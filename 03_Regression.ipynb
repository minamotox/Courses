{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f059a-df16-447d-8c9d-72a3761f5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f504be-afc4-48c1-80f9-17ed40dd5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.executable #sys.version_info\n",
    "sys.version\n",
    "print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98904c81-78c7-4d94-9534-e8d211c1c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "\n",
    "print('python:',sys.version)\n",
    "print('Numpy:',np.__version__)\n",
    "print('Pandas:',pd.__version__)\n",
    "print('Scikitlearn:',sklearn.__version__)\n",
    "print('Seaborn: ',sns.__version__)\n",
    "print('matplotlib:',matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8d75a-4920-410f-82ce-b875f25172ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Boston=pd.read_csv('Data/housing.data',delim_whitespace=True, header=None)\n",
    "Boston.columns=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX',\n",
    "                'PTRATIO','B','LSTAT','MEDV']\n",
    "#Boston.to_csv('Boston.csv')\n",
    "Boston.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c3db8-88dd-47a8-b157-77e22fb826e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table style=\"width:60%\">\n",
    "<tr>\n",
    "<th> Code :   </th>\n",
    "<th> Description : </th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th> CRIM </th> \n",
    "<th> per capita crime rate per town  </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th> ZN </th>\n",
    "<th> proportion of residential land zones for lots over 25000 sgr ft ></th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th> INDUS </th>\n",
    "<th> proportion of non retail business acr per town </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th> CHAS</th>\n",
    "<th> Charles River dummy variable (=1 if tract bounds river) </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th> NOX </th>\n",
    "<th> NO concentration (pp 10 million) </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th> RM </th>\n",
    "<th> Average N rooms  </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>  AGE  </th>\n",
    "<th> proportion of owner occupied builts prior 1940 </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>  DIS  </th>\n",
    "<th> weighted distances to 5 Boston employment centers </th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th>  RAD  </th>\n",
    "<th> index of accessibility to radial highways </th>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<th>  TAX  </th>\n",
    "<th> full value property tax rate per 10 000 $ </th>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<th>  PTRATIO </th>\n",
    "<th> pupil teacher ratio by town </th>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<th> B </th>\n",
    "<th> 1000(BK-0.63)² ; bk proportion of blacks by town </th>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<th> LSTAT </th>\n",
    "<th> % lower status of the population </th>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<th>  MEDV  </th>\n",
    "<th> median value of owner occupied homes in 1000 $ </th>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b0bd3-6d27-4256-9082-e3d3ee7838de",
   "metadata": {},
   "source": [
    "# Exploratory data Analysis (EDA) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8cf5f-873c-4cf3-8ac0-d8a165b77792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.pairplot(Boston, height=1.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a02af-bd87-4704-9798-3b3d91d3fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Features=['CRIM','ZN','INDUS','NOX','RM']\n",
    "sns.pairplot(Boston[Features], height=2.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044e487-caaa-4f7d-945a-4a230cb51777",
   "metadata": {},
   "outputs": [],
   "source": [
    "Features2=['AGE','TAX','PTRATIO','B','LSTAT','MEDV']\n",
    "sns.pairplot(Boston[Features2], height=2.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0647e50-cbb4-4c16-8d7c-24e0069f653a",
   "metadata": {},
   "source": [
    "# Correlation & Features selection #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af57ec-065d-4909-854f-9f16d42f4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format='{:,.3f}'.format\n",
    "Boston.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8037da-a400-4312-8ac9-71a2334e1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.heatmap(Boston.corr(), annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64f13e-2dcd-426f-9a6c-fba2d378ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(Boston[['CRIM','ZN','INDUS','CHAS','MEDV']].corr(), annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b07f55-7b1e-42f9-adb4-b8f2ced365ae",
   "metadata": {},
   "source": [
    "# Linear regression with Scikit-Learn #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa4019-8c3b-4228-9f55-ef3c4ccc0f95",
   "metadata": {},
   "source": [
    "## Five steps of in using Scikit-earn estimator API by Jacob T. Vanderplas: ##\n",
    "<ol>\n",
    "<li> Choose class model by importing the appropriate estimator class from Scikit-earn </li>\n",
    "<li> Choose model hyperparameters by instanciating this class with desired values\n",
    "<li> Arrange data into a features matrix and target vector\n",
    "<li> Fit the model to ur data by calling the fit() method of the model instance\n",
    "<li> Apply the model to the new data:\n",
    "<ul>\n",
    "<li> for supervised learning: often we predict labels for unknown data using predict() method\n",
    "<li> for unsupervised learning: often we transform or infer properties of the data using the transform() or predict() method\n",
    "</ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2cd88-87ab-40d7-87fb-44b34e92f58b",
   "metadata": {},
   "source": [
    "### MEDV vs RM Linear regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566208fc-dc41-4ec9-9e5b-4a095544e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X=Boston['RM'].values.reshape(-1,1)\n",
    "Y=Boston['MEDV'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74699da-0572-46d9-b171-67089b0c2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LinearRegression()\n",
    "model.fit(X,Y)\n",
    "print(f'linear model coeficient={model.coef_} and B={model.intercept_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f0299-cfaf-44ae-821d-030d045c1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x=X,y=Y,color='orange')\n",
    "plt.xlabel('Average N of rooms')\n",
    "plt.ylabel('owner occupied houses mediane value 1000$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace6087-fe84-44ef-a9e9-0fa6ca5d6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='RM', y='MEDV', data=Boston, kind='reg',height=10,color='orange')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6fc2e-d0bf-4418-adf2-8f4841654de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.predict(np.array([5]).reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8dcbdf-0a4e-420a-b05f-5f072ec39d3f",
   "metadata": {},
   "source": [
    "### LSTAT vs MEDV Linear regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491c603-415b-420f-82b3-edb9a3d8d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X2=Boston['LSTAT'].values.reshape(-1,1)\n",
    "Y2=Boston['MEDV'].values\n",
    "model2=LinearRegression()\n",
    "model2.fit(X2,Y2)\n",
    "print(f'linear, a={model2.coef_} and b={model2.intercept_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168321fb-54b3-4445-934d-3ef79bc28beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x=X2,y=Y2,color='DeepSkyBlue');\n",
    "plt.xlabel('Median value of owner occupied homes in 1000 $')\n",
    "plt.ylabel('% lower status of the population')\n",
    "plt.grid()\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55977980-40b5-49da-90ff-1b07f55d6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional\n",
    "sns.jointplot(x='LSTAT', y='MEDV', data=Boston, kind='reg',height=10,color='lime')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c29740-923b-4459-8dce-15f5b29a7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.predict(np.array([15]).reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee42e92-41c6-4036-b926-a42f4bbd5831",
   "metadata": {},
   "source": [
    "# Robust Regression #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3fd359-3138-40f8-9dff-d1ce1aefb8b8",
   "metadata": {},
   "source": [
    "## Random Sample Consensus (RANSAC) ##\n",
    "<ol>\n",
    "<li> select <strong> min_samples </strong>random samples from original data and check whether the dataset is valid <strong>(is_data_valid)</strong> </li>\n",
    "<li> fit a model to a random subset <strong>(base_estimator.fit)</strong> and check whether the estimated model is valid <strong> (is_model_valid)</strong> </li>\n",
    "<li> Classify all data as inliers or ouliers by calculating the residuals to the estimated model <strong>(base_estimator.predict(x)-y)</strong> all data samples\n",
    "with absolute residuals smaller than the <strong>residual_threshold </strong>are considered as inliers </li>\n",
    "<li> save fitted model as best model if number of outliers samples is maximal, in case the current estimated model has the same number of inliers,\n",
    "    it is considered as the best model if it has better score</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec65a32-9d00-4cca-bb5d-c246c99ce1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "#RM vs MEDV\n",
    "#X=Boston['RM'].values.reshape(-1,1)\n",
    "#Y=Boston['MEDV'].values\n",
    "coef=True\n",
    "# Robustly fit linear model with RANSAC algorithm\n",
    "ransac = linear_model.RANSACRegressor()\n",
    "ransac.fit(X,Y)\n",
    "inlier_mask = ransac.inlier_mask_\n",
    "outlier_mask = np.logical_not(inlier_mask)\n",
    "\n",
    "# Compare estimated coefficients\n",
    "print(f'Estimated coefficients (coef:{coef} , linear regression: {model.coef_},RANSAC= {ransac.estimator_.coef_}x+{ransac.estimator_.intercept_})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2a484-fd62-4793-8fb9-af9c282062f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions and plotting\n",
    "line_x=np.arange(3,10,1)\n",
    "line_y_ransac=ransac.predict(line_x.reshape(-1,1))\n",
    "line_y=model.predict(line_x.reshape(-1,1))\n",
    "\n",
    "sns.set(style='darkgrid', context='notebook')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[inlier_mask], Y[inlier_mask], c='blue', marker='o', label='Inliers')\n",
    "plt.scatter(X[outlier_mask], Y[outlier_mask], c='violet', marker='s', label='Outliers')\n",
    "plt.plot(line_x, line_y, color='grey',linewidth=2, label=\"Linear regressor\")\n",
    "plt.plot(line_x,line_y_ransac,color=\"red\", linewidth=2,label=\"RANSAC regressor\")\n",
    "plt.xlabel('Average N rooms')\n",
    "plt.ylabel('median value of owner occupied homes in 1000 $ ')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13b4df-ccd3-4f80-856a-c64d943c5872",
   "metadata": {},
   "source": [
    "**linear regression Assumptions:** https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2889f14b-e164-4bdb-9620-10514469b9ed",
   "metadata": {},
   "source": [
    "# Evaluate Regression Model Performance #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcfea6e-2001-4357-80ed-4935611ff9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=Boston.iloc[:, :-1].values\n",
    "Y=Boston['MEDV'].values\n",
    "\n",
    "x_train, x_test,y_train, y_test=train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "lr=LinearRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "y_train_pred=lr.predict(x_train)\n",
    "y_test_pred=lr.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b42596-7f9b-48c0-86a6-0ec6971f1db3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Method1: Residual Analysis ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40fa90-984b-4d49-bdc5-e59568c0d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(y_train_pred, y_train_pred-y_train, c='blue', marker='s', label='Training')\n",
    "plt.scatter(y_test_pred,y_test_pred-y_test, c='orange', marker='o', label='Test')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals ')\n",
    "plt.legend(loc='upper right')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='k')\n",
    "plt.xlim([-10,50])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd0362-667a-4ede-aed0-82dc69b065e1",
   "metadata": {},
   "source": [
    "### Method2: Mean Squarred Error (MSE) ###\n",
    "<ul>\n",
    "<li>The average value of the sums of the squarred error cost function </li>\n",
    "<li>Useful for comparing different regression models </li>\n",
    "<li>For tuning parameters via a grid search and cross-validation </li>\n",
    "</ul>\n",
    "\n",
    "## $$MSE={{1}\\over{n }}{\\sum_{i=1}^n{(yi-ŷi)²}}$$ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61066b3d-c5e9-49ca-9e39-77da3cae0523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_train,y_train_pred)\n",
    "mean_squared_error(y_test,y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e5ecdf-8a20-4847-8ded-6de7c6fcb6c0",
   "metadata": {},
   "source": [
    "### Method3: Coefficient of determination R² ###\n",
    "### $$ R²=1-{SSE \\over SST}$$ ###\n",
    "<ul>\n",
    "<li>SSE: Sum of squarred errors </li>\n",
    "<li>SST: Total sum of squares </li>\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2133c8-716a-49f9-bf1b-4bbe67079112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_train,y_train_pred)\n",
    "r2_score(y_test,y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa7ea84-3ec7-438c-9053-edf6e670bb00",
   "metadata": {},
   "source": [
    "# The perfect model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b71ffc-ffe5-4a0f-89b1-18bebe3594da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "generate_random=np.random.RandomState(0)\n",
    "x=10*generate_random.rand(1000)\n",
    "y=3*x+np.random.randn(1000)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(x,y, marker='^', c='indigo')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f9cad-8a19-4cfa-87a4-04df9e353d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test,y_train, y_test=train_test_split(x,y, test_size=0.3, random_state=0)\n",
    "model=LinearRegression()\n",
    "model.fit(x_train.reshape(-1,1),y_train)\n",
    "y_train_pred=model.predict(x_train.reshape(-1,1))\n",
    "y_test_pred=model.predict(x_test.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718bb02-92be-4049-b88c-e912f198309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual Analysis\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(y_train_pred, y_train_pred-y_train, c='blue', marker='s', label='Training')\n",
    "plt.scatter(y_test_pred,y_test_pred-y_test, c='orange', marker='o', label='Test')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals ')\n",
    "plt.legend(loc='upper right')\n",
    "plt.hlines(y=0, xmin=3, xmax=33, lw=2, color='r')\n",
    "plt.xlim([-5,35])\n",
    "plt.ylim([-25,15])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14a5b4-033d-49fb-be61-65589ad6b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_train,y_train_pred)\n",
    "mean_squared_error(y_test,y_test_pred)\n",
    "print(f'MSE train:{mean_squared_error(y_train,y_train_pred)} , MSE Test:{mean_squared_error(y_test,y_test_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ee497-5fcd-493d-b7c5-161eb49fc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R²\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_train,y_train_pred)\n",
    "r2_score(y_test,y_test_pred)\n",
    "print(f'R² train:{mean_squared_error(y_train,y_train_pred)} , R² Test:{mean_squared_error(y_test,y_test_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cac56-3810-474f-bfb8-f222701495bd",
   "metadata": {},
   "source": [
    "# Multiple Regression #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295c83b-373c-428a-9be5-554aad3d0c43",
   "metadata": {},
   "source": [
    "## y=$\\beta$0+$\\beta1$*x1+$\\beta2$*x2... ##\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c4012-6106-4402-8d35-32c772133bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Housing=pd.read_csv('housing.data',delim_whitespace=True, header=None)\n",
    "Housing.columns=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX',\n",
    "                'PTRATIO','B','LSTAT','MEDV']\n",
    "Boston=Housing.iloc[:,0:13]\n",
    "\n",
    "X=Boston\n",
    "Y=Housing.iloc[:,13:15]\n",
    "print(f'x= \\n {X} and y=\\n {Y}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ecb1b-8ff9-454e-96ec-e6c1e94aeb6a",
   "metadata": {},
   "source": [
    "# Statsmodels #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95072027-dd11-4b94-8ec7-b39f95907153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_constant=sm.add_constant(X)\n",
    "pd.DataFrame(X_constant)\n",
    "#sm.OLS?\n",
    "model=sm.OLS(Y,X_constant)\n",
    "lr=model.fit()\n",
    "lr.summary()\n",
    "#P must be <0.025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2d685-13c9-4652-94a2-f72de44f4817",
   "metadata": {},
   "source": [
    "## Summary: ##\n",
    "<ul>\n",
    "<li> <b>coefficient:</b> it is the value of the intercept. For each variable, it is the measurement of how change in that variable affects the independent variable. It is the ‘m’ in ‘y = mx + b’ One unit of change in the dependent variable will affect the variable’s coefficient’s worth of change in the independent variable. </li>\n",
    "<li><b>std error:</b> is an estimate of the standard deviation of the coefficient, a measurement of the amount of variation in the coefficient throughout its data points. A low std error compared to a high coefficient produces a high t statistic, which signifies a high significance for your coefficient.</li>\n",
    "<li><b>P>|t|</b> is one of the most important statistics in the summary. It uses the t statistic to produce the p-value, a measurement of how likely your coefficient is measured through our model by chance. The p-value of 0.378 for Wealth is saying there is a 37.8% chance the Wealth variable has no affect on the dependent variable, Lottery, and our results are produced by chance. A common alpha is 0.05, which few of our variables pass in this instance.</li>\n",
    "<li><b>[0.025 and 0.975]</b> are both measurements of values of our coefficients within 95% of our data, or within two standard deviations. Outside of these values can generally be considered outliers.</li> </ul>\n",
    "The p-value is the smallest test size that would cause an observation of t=0.1 to lead to a rejection of the null hypothesis\n",
    "\n",
    "### Residual Tests: ###\n",
    "<ul>\n",
    "<li><b>Omnibus:</b> a combined statistic test for skewness and kurtosis</li>\n",
    "<li><b>prob(Omnibus):</b> P-value of Omnibus test</li>\n",
    "<li><b>Skewness:</b> a measure of symmetry of residuals around the mean.Zero if symmetrical. A positive value indicates a long tail to the right, a negative value indicates a long tail to the left</li>\n",
    "<li><b>Kurtosis:</b> A measure of the shape of distribution of the residuals, A normal distribution has 0 measure.A negative value points to a flatter than normal distribution, a positive one has a higher peak than normal distribution</li>\n",
    "<li><b>Durbin-Watson:</b> A test for presence of correlation among the residuals, this is important for time series modelling</li>\n",
    "<li><b>Jarque-Bera:</b> it is a combined statistical test of Skewness and Kurtosis</li>\n",
    "<li><b>Prob(JB): </b>p_value of Jarque-Bera</li>\n",
    "<li><b>Cond.No: </b>it is a test for multicolinearity. > 30 indicates unstable results.</li>\n",
    "    \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652a236-c896-4eac-80b0-678449d4067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "form_lr=smf.ols(formula='Y ~ CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT',data=Housing)\n",
    "mlr=form_lr.fit()\n",
    "mlr.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce5ee3-0939-4614-b8fa-2ad2f08aafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting 100 rows in basis of CRIM and Black\n",
    "model_ex=smf.ols(formula='Y ~ CRIM+B',data=Housing) #CRIM+ZN+CHAS+\n",
    "mlr_ex=model_ex.fit()\n",
    "mlr_ex.summary()\n",
    "#predictions = mlr_ex.predict(Housing[0:100])\n",
    "#predictions.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce83266-9490-4822-8fa7-b0f7407ee1b5",
   "metadata": {},
   "source": [
    "# Correlation Matrix #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac1229-dd2c-48cb-8cce-fcba1ebbcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Housing=pd.read_csv('Data/housing.data',delim_whitespace=True, header=None)\n",
    "Housing.columns=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX',\n",
    "                'PTRATIO','B','LSTAT','MEDV']\n",
    "Boston=Housing.iloc[:,0:13]\n",
    "X=Boston\n",
    "Y=Housing.iloc[:,13:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc0cfc-0329-4e70-b709-4ee69847aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format='{:,.2f}'.format\n",
    "corr_matrix=Boston.corr()\n",
    "corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e651ef9-c0f6-4738-845b-6aceff6704aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[np.abs(corr_matrix)< 0.6]=0 #lesser than 0.6 and greater than -0.6\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a393f19-f788-4500-bde0-24cd32dd0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette('tab20b',10) # Default color palette\n",
    "sns.palplot(palette) # Plotting your palette!\n",
    "#sns.palplot(sns.color_palette('husl', 20)) # Seaborn color palette, with number of colors \n",
    "#sns.color_palette('rocket', as_cmap=True) # Get a CMap\n",
    "\n",
    "plt.figure(figsize=(18,9))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=palette)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050732d-27b1-4bac-b05e-81a91ddb1b6f",
   "metadata": {},
   "source": [
    "# Detecting colinearity with Eigenvectors #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c1903-4ac8-41f5-b206-312545a48fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues , eigenvectors= np.linalg.eig(Boston.corr())\n",
    "pd.Series(eigenvalues).sort_values()\n",
    "np.abs(pd.Series(eigenvectors[:,8])).sort_values(ascending=False)\n",
    "print(Boston.columns[2],Boston.columns[8],Boston.columns[9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c75e12-f89c-4578-80e6-0d0bb8bb0658",
   "metadata": {},
   "source": [
    "small values= presence of colinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a07849-50e3-4992-b626-1bbae1ffb300",
   "metadata": {},
   "source": [
    "# Revising Feature importance and Extractions #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880e1e18-de08-492f-b17f-57f3c5a01635",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Boston['TAX'])\n",
    "#plt.hist(Boston['NOX'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ca0df-3819-4df8-9001-e6e24697a721",
   "metadata": {},
   "source": [
    "# Standardise variable to identify Key Features #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b11dc-75a0-4567-809c-f2543d982e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pd.options.display.float_format='{:,.4f}'.format\n",
    "model=LinearRegression()\n",
    "model.fit(X,Y)\n",
    "mc=list(np.transpose(model.coef_))\n",
    "bc=list((Boston.columns))\n",
    "bcc=np.transpose(bc)\n",
    "result=pd.DataFrame({'Name':bc,'Coefficient':mc}).set_index('Name')\n",
    "co=np.abs(result).sort_values(by=['Coefficient'], ascending=False)\n",
    "co\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bec2e-7aac-4176-ae81-92adec0507ef",
   "metadata": {},
   "source": [
    "### 2nd method ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c25c8b-0eb8-4bfa-9e99-9e6630bfaac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "scaler=StandardScaler()\n",
    "standard_coefficient_linear_reg=make_pipeline(scaler,model)\n",
    "standard_coefficient_linear_reg.fit(X,Y)\n",
    "scl=list(np.transpose(standard_coefficient_linear_reg.steps[1][1].coef_))\n",
    "result=pd.DataFrame({'Name':bcc,'Coefficient':scl}).set_index('Name')\n",
    "co=np.abs(result).sort_values(by=['Coefficient'], ascending=False)\n",
    "co\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2525c38d-6851-40ea-b36e-7446e2d1f122",
   "metadata": {},
   "source": [
    "## Use R² to identify key features ##\n",
    "<ul>\n",
    "<li>Compare R² of the model with R² without a feature</li>\n",
    "<li>significant change in R² means the importance of the feature</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfca82a-7d36-4738-ae34-60b1b5383f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "linear_reg=smf.ols(formula='Y ~ CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT',data=Boston)\n",
    "Benchmark=linear_reg.fit()\n",
    "r2_score(Y,Benchmark.predict(Boston))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2910b-c56a-4f0c-8ae1-0caed7d5b687",
   "metadata": {},
   "source": [
    "## Without LSTAT ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712da64-1e91-4468-a99d-9e792a7525b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg=smf.ols(formula='Y ~ CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B',data=Boston)\n",
    "lr_lstat=linear_reg.fit()\n",
    "r2_score(Y,lr_lstat.predict(Boston))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddefbeec-dca2-4fa8-b802-ee4c3f122ada",
   "metadata": {},
   "source": [
    "## Without AGE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e88f4b-f560-49e3-b2bb-cdaf121cef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg=smf.ols(formula='Y ~ CRIM+ZN+INDUS+CHAS+NOX+RM+DIS+RAD+TAX+PTRATIO+B+LSTAT',data=Boston)\n",
    "lr_AGE=linear_reg.fit()\n",
    "r2_score(Y,lr_AGE.predict(Boston))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0fb676-fdae-4d7a-8b94-759e4016c7a3",
   "metadata": {},
   "source": [
    "# Regularized Regression #\n",
    "<ul>\n",
    "<li>Ridge regression </li>\n",
    "<li>Least absolute shrinkage and selection operator(LASSO) </li>\n",
    "<li>Elastic net </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df4ae0-2bae-43c2-afc7-78acdfbf490b",
   "metadata": {},
   "source": [
    "## Ridge Regression ##\n",
    "\n",
    "<p>Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. \n",
    "This method performs <b>L2</b> regularization. When the issue of multicollinearity occurs, least-squares are unbiased, \n",
    "and variances are large, this results in predicted values being far away from the actual values. \n",
    "\n",
    "The cost function for ridge regression: </p>\n",
    "\n",
    "## $$ Min(||X( \\omega)-Y||_2^2 + \\lambda||\\omega||_2^2) $$ ##\n",
    "\n",
    "<p>Lambda is the penalty term. λ given here is denoted by an alpha parameter in the ridge function. So, by changing the values of alpha, we are controlling the penalty term. The higher the values of alpha, the bigger is the penalty and therefore the magnitude of coefficients is reduced.\n",
    "<ul>\n",
    "<li>   It shrinks the parameters. Therefore, it is used to prevent multicollinearity </li> \n",
    "<li>    It reduces the model complexity by coefficient shrinkage </li> \n",
    "<li> Check out the free course on regression analysis. </li> \n",
    "</ul>\n",
    "<b>Ridge Regression Models </b> \n",
    "\n",
    "For any type of regression machine learning model, the usual regression equation forms the base which is written as: </p>\n",
    "\n",
    "<div>\n",
    "$$ Y = X\\beta+e $$\n",
    "    </div>\n",
    "<p>\n",
    "Where Y is the dependent variable, X represents the independent variables, B is the regression coefficients to be estimated, and e represents the errors are residuals. \n",
    "Once we add the lambda function to this equation, the variance that is not evaluated by the general model is considered. After the data is ready and identified to be part of L2 regularization, there are steps that one can undertake.</p>\n",
    "<b>Standardization </b>\n",
    "<p>\n",
    "In ridge regression, the first step is to standardize the variables (both dependent and independent) by subtracting their means and dividing by their standard deviations. This causes a challenge in notation since we must somehow indicate whether the variables in a particular formula are standardized or not. As far as standardization is concerned, all ridge regression calculations are based on standardized variables. When the final regression coefficients are displayed, they are adjusted back into their original scale. However, the ridge trace is on a standardized scale.\n",
    "\n",
    "Also Read: Support Vector Regression in Machine Learning </p>\n",
    "<b>Bias and variance trade-off</b>\n",
    "\n",
    "Bias and variance trade-off is generally complicated when it comes to building ridge regression models on an actual dataset. However, following the general trend which one needs to remember is:\n",
    "\n",
    "    The bias increases as λ increases.\n",
    "    The variance decreases as λ increases.\n",
    "\n",
    "<b>Assumptions of Ridge Regressions</b>\n",
    "\n",
    "The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence. However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed.\n",
    "Now, let’s take an example of a linear regression problem and see how ridge regression if implemented, helps us to reduce the error.\n",
    "\n",
    "We shall consider a data set on Food restaurants trying to find the best combination of food items to improve their sales in a particular region. \n",
    "<p>\n",
    "if lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen. This technique works very well to avoid over-fitting issue.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec5205-2129-4df2-beae-b2bd1479bec1",
   "metadata": {},
   "source": [
    "## LASSO ##\n",
    "\n",
    "<p>Is a linear model that estimates sparse coefficients. Called L1 regularization.\n",
    "Mathematically, it consists in a linear model trained with $\\phi1$ prior as regularized.The objective function to minimize is:</p>\n",
    "\n",
    "## $$ Min_\\omega \\frac{1}{2n_{samples}}||X( \\omega)-Y||_2^2 + \\lambda||\\omega||_1 $$ ##\n",
    "<p> The LASSO estimate thus solves of least squares penalty with $\\lambda||\\omega||_1$ added, where $\\lambda$ is constant and $||\\omega||_1$ is the $\\phi1$ -norm of the parameter vector.</p>\n",
    "<p>\n",
    "The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a357087-205a-4501-b1b9-345e25b578fd",
   "metadata": {},
   "source": [
    "## ELASTIC Net ##\n",
    "\n",
    "<p>A linear regression model trained with L1 and L2 prior as regularizer.\n",
    "This combination allows for learning a sparse model where few of the weights are non 0 like LASSO, while still maintaining the regularization properties of ridge.\n",
    "<b>ELASTIC NET</b> is useful when there are multiple features which are correlated with one another.<b>LASSO</b> is likely to pick one of these randomly, while <b>ELASTIC NET</b> is likely to pick both.\n",
    "A practical advantage of trading-off between LASSO and Ridge is it allows <b>ELASTIC NET</b> to inherit some of Ridge's stability under rotation.\n",
    "the objective function to minimize is in this case:\n",
    "</p>\n",
    "\n",
    "## $$ Min_\\omega \\frac{1}{2n_{samples}}||X( \\omega)-Y||_2^2 + \\lambda p||\\omega||_1 + \\frac{\\lambda(1-p)}{2} ||\\omega||_2^2$$ ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87b875-af1e-41c7-beca-ee94dbbdcc30",
   "metadata": {},
   "source": [
    "# Outliers Impact #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611ecae-b5c7-4d16-aa21-b50c26d22cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766714be-13f1-4dec-8e44-48aecfd7fe03",
   "metadata": {},
   "source": [
    "## Linear regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2a4a4-6d3a-43be-9a0e-35df312d0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples=100\n",
    "rng=np.random.randn(n_samples)*10\n",
    "y_gen=0.5*rng+2*np.random.randn(n_samples)\n",
    "lr=LinearRegression()\n",
    "model=lr.fit(rng.reshape(-1,1), y_gen)\n",
    "model_pred=lr.predict(rng.reshape(-1,1))\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='b')\n",
    "plt.plot(rng,model_pred, color='r' )\n",
    "print(lr.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac2ae9-fc8a-49ab-9385-850a4340bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=rng.argmax()\n",
    "y_gen[idx]=200\n",
    "idx=rng.argmin()\n",
    "y_gen[idx]=-200\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='green')\n",
    "o_lr=LinearRegression()\n",
    "o_lr.fit(rng.reshape(-1,1), y_gen)\n",
    "o_model_predict=o_lr.predict(rng.reshape(-1,1))\n",
    "plt.scatter(rng, y_gen, color='grey')\n",
    "plt.plot(rng,o_model_predict, color='indigo' )\n",
    "print(o_lr.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03076e24-1894-41f3-bac5-aa0be451f4f0",
   "metadata": {},
   "source": [
    "## Ridge Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf0cc3-8664-4af3-8b07-182a95063b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#rng_N=preprocessing.normalize(rng.reshape(-1,1))\n",
    "ridge_mod=Ridge(alpha=0.5, fit_intercept=True)#, normalize=True)\n",
    "ridge_mod.fit(rng.reshape(-1,1), y_gen)\n",
    "ridge_mod_pred=ridge_mod.predict(rng.reshape(-1,1))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='red')\n",
    "plt.plot(rng,ridge_mod_pred, color='blue' )\n",
    "ridge_mod.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea26c4e-9176-4ee6-9a9c-b4f0804206da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge + normalize data ???\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge_modN = make_pipeline(StandardScaler(with_mean=False),RidgeCV())\n",
    "ridge_modN.fit(rng.reshape(-1,1), y_gen)\n",
    "ridge_modN_pred=ridge_modN.predict(rng.reshape(-1,1))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='red')\n",
    "plt.plot(rng,ridge_modN_pred, color='blue' )\n",
    "\n",
    "ridge_modN['ridgecv'].coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f474f17-29e0-4490-834d-a660ae52a40f",
   "metadata": {},
   "source": [
    "## LASSO ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ca8e0-9fc2-40cc-87c9-8c6fe7ba2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "Lasso_mod=Lasso(alpha=0.5, fit_intercept=True)#, normalize=True)\n",
    "Lasso_mod.fit(rng.reshape(-1,1), y_gen)\n",
    "Lasso_mod_pred=Lasso_mod.predict(rng.reshape(-1,1),)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='green')\n",
    "plt.plot(rng,Lasso_mod_pred, color='blue' )\n",
    "Lasso_mod.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b3d470-a888-460e-97c5-4a4fa8482a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "lasso_pipeline = pipeline(steps=[('preprocess', Lasso_mod),('model', Lasso)])\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='red')\n",
    "plt.plot(rng,Lasso_modN_prod, color='blue' )\n",
    "\n",
    "Lasso_modN['model'].coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3472be65-6f62-4959-ae04-507c0a378b0f",
   "metadata": {},
   "source": [
    "## Elastic Net regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11a344-26f3-4692-8f11-d2dd60b1c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "en_model=ElasticNet(alpha=0.5, fit_intercept=True)#, normalize=True)\n",
    "en_model.fit(rng.reshape(-1,1), y_gen)\n",
    "en_model_pred=en_model.predict(rng.reshape(-1,1),)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='orange')\n",
    "plt.plot(rng,en_model_pred, color='blue' )\n",
    "en_model.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c9e69-b016-4130-bdc2-24521a47e411",
   "metadata": {},
   "source": [
    "### When to use Ridge, Lasso or Elasticnet? ###\n",
    "<ul>\n",
    "<li><b>Ridge Regression</b> can't zero out coefficient, you either end up including all the coefficients in the model, or none of them.</li>\n",
    "<li><b>LASSO</b> does both parameters shrinkage and variable selection automatically.</li>\n",
    "<li>if some of your covariates are highly correlated, you may want to look at the <b>ElasticNet</b> instead of <b>LASSO</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac91fa7-2a00-4c07-a7d0-7d04afc91ffc",
   "metadata": {},
   "source": [
    "# Polynomial Regression #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259338b-2b84-4b09-9841-2c0e9db21f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d348b7f-e832-462c-baa9-7417be7df0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "\n",
    "print('python:',sys.version)\n",
    "print('Numpy:',np.__version__)\n",
    "print('Pandas:',pd.__version__)\n",
    "print('Scikitlearn:',sklearn.__version__)\n",
    "print('Seaborn: ',sns.__version__)\n",
    "print('matplotlib:',matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab8881-46f7-440d-a78d-c06ff0f5b508",
   "metadata": {},
   "source": [
    "## $$ y= x^3+100+\\epsilon $$ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39d1bb-9d5c-4d60-8140-7ec31a47414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples=100\n",
    "X=np.linspace(0,10,100)\n",
    "rng=np.random.randn(n_samples)*100\n",
    "Y=X**3+100+rng\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X,Y,c='#7f7f7f' )\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff32273-490e-42d9-af8b-cfbe451cc151",
   "metadata": {},
   "source": [
    "### Linear Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb9ff0-0809-4c75-89d8-7889fe68d82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "lr=LinearRegression()\n",
    "lr.fit(X.reshape(-1,1),Y)\n",
    "lr_pred=lr.predict(X.reshape(-1,1))\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X,Y,c='#139fe8' )\n",
    "plt.plot(X,lr_pred, c='#8a010d' )\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(f'model Coef={lr.coef_}, R² ={r2_score(Y,lr_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f42a18-a8a2-4772-b117-7b6abc0df70c",
   "metadata": {},
   "source": [
    "### Polynomial Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ae8f2-c3e4-4420-9828-11c6ea9886a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_reg=PolynomialFeatures(degree=2)\n",
    "X_poly=poly_reg.fit_transform(X.reshape(-1,1))\n",
    "lr2=LinearRegression()\n",
    "lr2.fit(X_poly,Y.reshape(-1,1))\n",
    "Y_pred=lr2.predict(X_poly)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X,Y,c='#14047b' )\n",
    "plt.plot(X,Y_pred, c='red' )\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(f'model Coef={lr2.coef_}, R² ={r2_score(Y,Y_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2f257-a16a-4d68-968d-14b4a8486fac",
   "metadata": {},
   "source": [
    "### Example: Boston dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b47971-dade-4a2d-919f-83bbe9f1d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "Boston=pd.read_csv('housing.data',delim_whitespace=True, header=None)\n",
    "Boston.columns=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX',\n",
    "                'PTRATIO','B','LSTAT','MEDV']\n",
    "\n",
    "pd.options.display.float_format='{:,.3f}'.format\n",
    "sns.pairplot(Boston, size=1.5)\n",
    "Boston.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7260c65f-0f92-427b-857f-f453a9546e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_boston=np.array(Boston['DIS'])\n",
    "Y_boston=np.array(Boston['NOX'])\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_boston,Y_boston)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2dd493-9d7a-4a5e-83b4-bff59a934151",
   "metadata": {},
   "source": [
    "### linear Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711fee2-aafd-4380-be3d-9af98047aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin=LinearRegression()\n",
    "lin.fit(X_boston.reshape(-1,1),Y_boston)\n",
    "lin_pred=lin.predict(X_boston.reshape(-1,1))\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X_boston,Y_boston,c='darkslateblue' )\n",
    "plt.plot(X_boston,lin_pred, c='fuchsia' )\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(f'model Coef={lin.coef_}, R² ={r2_score(Y_boston,lin_pred):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d2191-2191-406d-abc3-144fd9d35e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly=PolynomialFeatures(degree=2)\n",
    "X_poly=poly.fit_transform(X_boston.reshape(-1,1))\n",
    "poly2=LinearRegression()\n",
    "poly2.fit(X_poly,Y_boston.reshape(-1,1))\n",
    "X_fit=np.arange(X_boston.min(), X_boston.max(),1)[:,np.newaxis]\n",
    "Y_pred=poly2.predict(poly.fit_transform(X_fit.reshape(-1,1)))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_boston,Y_boston, c='forestgreen')\n",
    "plt.plot(X_fit,Y_pred,c='lightsalmon', linewidth=3)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(f'model Coef={poly2.coef_}, R² ={r2_score(Y_boston,poly2.predict(X_poly)):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31297e-5e6a-4052-b314-67d51860569c",
   "metadata": {},
   "source": [
    "### Cubic Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a561fdf2-1b34-442a-a98c-7e00db90fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_3=PolynomialFeatures(degree=3)\n",
    "X_poly=poly_3.fit_transform(X_boston.reshape(-1,1))\n",
    "poly3=LinearRegression()\n",
    "poly3.fit(X_poly,Y_boston.reshape(-1,1))\n",
    "X_fit=np.arange(X_boston.min(), X_boston.max(),1)[:,np.newaxis]\n",
    "Y_pred3=poly3.predict(poly_3.fit_transform(X_fit.reshape(-1,1)))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_boston,Y_boston, c='forestgreen')\n",
    "plt.plot(X_fit,Y_pred3,c='lightsalmon', linewidth=3)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(f'model Coef={poly3.coef_}, R² ={r2_score(Y_boston,poly3.predict(X_poly)):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5af61-a34e-434b-84fc-2837c1aa550d",
   "metadata": {},
   "source": [
    "# Non linear relationships #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543e33e-afe0-4b56-8913-c1cc7134a896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682aab30-efe0-4083-8b85-6352d4c469e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Data=pd.read_csv('Data/housing.data',delim_whitespace=True, header=None)\n",
    "Boston=pd.DataFrame(data=np.array(Data),index=None, columns=('CRIM','ZN','INDUS','CHAS',\n",
    "                                                             'NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV'), )#.iloc[:,:-1]\n",
    "\n",
    "#sns.pairplot(Boston, height=1.5)\n",
    "#plt.savefig('CorrMatrix.png')\n",
    "#pd.options.display.float_format='{:,.3f}'.format\n",
    "#Boston.corr()\n",
    "Boston\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7ef0b-f8d9-481c-828d-374219c0e83f",
   "metadata": {},
   "source": [
    "### Decision Tree ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45422694-22c7-4f4a-80c7-11accb130c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "Y=Boston['MEDV']\n",
    "X=Boston['LSTAT'].values\n",
    "tree=DecisionTreeRegressor(max_depth=5)\n",
    "tree.fit(X.reshape(-1,1),Y) \n",
    "sort_idx=X.flatten().argsort()\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[sort_idx],Y[sort_idx], c='b')\n",
    "plt.plot(X[sort_idx],tree.predict(X[sort_idx].reshape(-1,1)), color='r', linewidth=2)\n",
    "plt.xlabel('STAT')\n",
    "plt.ylabel('MEDV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9de82d-7bf8-43d7-8b8d-cdf5d0a79e8f",
   "metadata": {},
   "source": [
    "## Random Forest ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504dddd-eb7b-4587-a5aa-b36b8961b952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bee49c-d94c-49fc-8ea2-2f80e924beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=Boston\n",
    "Y=Boston['MEDV']\n",
    "X_train, X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,train_size=0.7, random_state=42) \n",
    "#Criterion: 'poisson', 'squared_error', 'friedman_mse', 'absolute_error'\n",
    "forest=RandomForestRegressor(n_estimators=500, criterion='friedman_mse', random_state=42, n_jobs=-1)\n",
    "forest.fit(X_train, Y_train)\n",
    "Y_train_pred=forest.predict(X_train)\n",
    "Y_test_pred=forest.predict(X_test)\n",
    "print(f'MSE Train={mean_squared_error(Y_train,Y_train_pred):.4f} , MSE Test={mean_squared_error(Y_test,Y_test_pred):.4f}')\n",
    "print(f'R² Train={r2_score(Y_train,Y_train_pred):.4f} , R² Test={r2_score(Y_test,Y_test_pred):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88844488-d4f4-455f-b464-401c130c5243",
   "metadata": {},
   "source": [
    "## AdaBoost ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc0cd3-a80e-4b0f-a605-566970a98c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ada=AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=500, random_state=42)\n",
    "ada.fit(X_train,Y_train)\n",
    "Y_train_predict=ada.predict(X_train)\n",
    "Y_test_predict=ada.predict(X_test)\n",
    "print(f'MSE Train={mean_squared_error(Y_train,Y_train_pred):.4f} , MSE Test={mean_squared_error(Y_test,Y_test_pred):.4f}')\n",
    "print(f'R² Train={r2_score(Y_train,Y_train_pred):.4f} , R² Test={r2_score(Y_test,Y_test_pred):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1fb18e-6cef-4f7c-9ba2-296d4f5973c6",
   "metadata": {},
   "source": [
    "### Feature importance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8958f-c5d3-4c89-a832-aacca9cb52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X=Boston\n",
    "Y=Boston['MEDV']\n",
    "X_train, X_test,Y_train,Y_test=train_test_split(X,Y, test_size=0.3, random_state=42) \n",
    "tree=DecisionTreeRegressor(max_depth=5)\n",
    "tree.fit(X,Y) #reshape(-1,1)\n",
    "#sort_idx=X.flatten().argsort()\n",
    "\n",
    "Y_train_pred=tree.predict(X_train)\n",
    "Y_test_pred=tree.predict(X_test)\n",
    "print(f'MSE Train={mean_squared_error(Y_train,Y_train_pred):.4f} ,MSE Test={mean_squared_error(Y_test,Y_test_pred):.4f}')\n",
    "print(f'R² Train={r2_score(Y_train,Y_train_pred):.4f} , R² Test={r2_score(Y_test,Y_test_pred):.4f}')\n",
    "\n",
    "result= pd.DataFrame(tree.feature_importances_, Boston.columns)\n",
    "result.columns=['Features']\n",
    "rt=result.sort_values(by='Features', ascending=False)\n",
    "rt.plot(kind='bar', figsize=(10,5), edgecolor='black')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc8984-2492-43a1-8393-068fe1cab6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "forest.feature_importances_\n",
    "result= pd.DataFrame(forest.feature_importances_, Boston.columns)\n",
    "result.columns=['Features']\n",
    "rt=result.sort_values(by='Features', ascending=False)\n",
    "rt.plot(kind='bar', figsize=(10,8), edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bf88b-b8c9-4b71-ad97-7cd58ab44b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ada\n",
    "ada.feature_importances_\n",
    "result= pd.DataFrame(ada.feature_importances_, Boston.columns)\n",
    "result.columns=['Features']\n",
    "rt=result.sort_values(by='Features', ascending=False)\n",
    "rt.plot(kind='bar', figsize=(10,4), edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2cf113-408a-440c-afb4-e368fb8f8c6d",
   "metadata": {},
   "source": [
    "# Data Preprocessing #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e0a4c-3832-4aac-8872-fada44d315b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.read_csv('Data/housing.data',delim_whitespace=True, header=None)\n",
    "Boston=pd.DataFrame(data=np.array(Data),index=None,\n",
    "                    columns=('CRIM','ZN','INDUS','CHAS','NOX',\n",
    "                             'RM','AGE','DIS','RAD','TAX','PTRATIO',\n",
    "                             'B','LSTAT','MEDV'), )#.iloc[:,:-1]\n",
    "\n",
    "#sns.pairplot(Boston, height=1.5)\n",
    "#plt.savefig('CorrMatrix.png')\n",
    "pd.options.display.float_format='{:,.3f}'.format\n",
    "co=Boston.corr()\n",
    "Boston\n",
    "#co['MEDV'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48eef54-39be-4a9e-8ecb-b57e6e73494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=pd.DataFrame(Boston['LSTAT'])\n",
    "X=Boston['LSTAT'].values\n",
    "Y=np.array(Boston['MEDV'])\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(X,Y)\n",
    "plt.show()\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406a68f-2bc6-4da5-8a91-907862a3f9cf",
   "metadata": {},
   "source": [
    "### Without Preprocessing ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b81c0-2b2c-4569-8ddc-0115964631c0",
   "metadata": {},
   "source": [
    "<p>Gradient descent is a widely used machine learning algorithm. It tells us how we can do better in predictive modeling <br>with an iterative approach. We will see how we can use the gradient descent algorithm to get better predicting results in linear regression.</p>\n",
    "<b> cost function in linear regression: </b><br>\n",
    " $$\\sum_{i=0}^n(yi-(\\beta1x+\\beta0))^2$$ \n",
    "<p>A cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between x and y. <br>\n",
    "The starting point is known as Learning Rate a constant (generally denoted as alpha ‘α’). The learning rate is a step to go down to attain convergence in minimum steps. At every step, the function will calculate the slope of a line and curve at that certain point. The convergence will attain when the slope will be equal to zero.</p>\n",
    "Gradient descent steps:\n",
    "<ol>\n",
    "<li>Adding a column of ones to x vector: \n",
    "  <b>x=np.c_[np.ones(x_RM.shape[0]),x_RM, x_LSTAT]</b></li>\n",
    "<li>Guess/Random θ: $$ \\theta (n+1*1)--> \\theta_0, \\theta_1,...\\theta_n  $$  </li> <br>\n",
    "we have x and θ_0, θ_1...θ_n  , we can do linear regression and predict the error \n",
    "<li>Predict the y values : </li>\n",
    " $$ pred^i=\\theta_0 * 1 + \\sum_{i=0}^n \\theta_i x_i   $$  \n",
    "<li>Calculate the error: calculate MSE \n",
    "     $$MSE={{1}\\over{n }}{\\sum_{i=1}^n{(yi-ŷi)²}}$$  </li>\n",
    "<li>Calculate the cost function :</li>\n",
    "  $$  cost=\\frac{1}{2m} \\sum_{i=1}^m (error)^2 $$ \n",
    "<li>Update θ :</li>\n",
    "    $$ \\theta=\\theta - \\alpha \\frac{1}{m} *\\sum_{i=1}^m (error)*x $$ \n",
    "<li>Repeat till the change in the cost function is negligible </li>\n",
    "<ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bcc5a4-8a6f-46a9-8a9d-8e5e8333a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Descent:\n",
    "\n",
    "alpha=0.0001\n",
    "w_=np.zeros(1+X.shape[0])\n",
    "cost_=[]\n",
    "n_=100\n",
    "for i in range (n_):\n",
    "    y_pred=np.dot(X,w_[1:])+w_[0]\n",
    "    errors=Y-y_pred\n",
    "    w_[1:]+=alpha+X.dot(errors)\n",
    "    w_[0]+=alpha+errors.sum()\n",
    "    cost=(errors**2).sum()/2.0\n",
    "    cost_.append(cost)\n",
    "    \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, n_+1),cost_)\n",
    "plt.xlabel('SSE')\n",
    "plt.ylabel('Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595353b-d4ff-44cc-94cc-e691af68a644",
   "metadata": {},
   "source": [
    "### With preprocessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4814ce-bbd4-45a1-b132-ab5a73657edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x=StandardScaler()\n",
    "sc_y=StandardScaler()\n",
    "X_std=sc_x.fit_transform(X.reshape(-1,1))\n",
    "Y_std=sc_y.fit_transform(Y.reshape(-1,1)).flatten()\n",
    "\n",
    "#Gradient Descent:\n",
    "alpha=0.0001\n",
    "w_=np.zeros(1+X_std.shape[1])\n",
    "cost_=[]\n",
    "n_=100\n",
    "for i in range (n_):\n",
    "    y_pred=np.dot(X_std[:,0:2],w_[1:])+w_[0]\n",
    "    errors=Y_std-y_pred\n",
    "    w_[1:]+=alpha*X_std.T.dot(errors)\n",
    "    w_[0]+=alpha*errors.sum()\n",
    "    cost=(errors**2).sum()/2.0\n",
    "    cost_.append(cost)\n",
    "    \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1,n_+1),cost_)\n",
    "plt.xlabel('SSE')\n",
    "plt.ylabel('Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cb5fb-e138-4750-9490-57087c1c5a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(X)\n",
    "plt.xlim(-40,40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e831d200-4660-4e6d-9c18-d5040ecc1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After scaling\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(X_std)\n",
    "plt.xlim(-4,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066b3aa-5185-4b88-a71a-810f0298a41b",
   "metadata": {},
   "source": [
    "Data pre-processing:\n",
    "<ul>\n",
    "<li>Standardisation / mean removal : mean =0 and variance= 1</li>\n",
    "<li>Normalization</li>\n",
    "<li>Binarization</li>\n",
    "</ul>\n",
    "<b>Assumptions:</b>\n",
    "    <ul>\n",
    "<li>Implicit / Explicit assumptions of ML algorithms: The features follow a normal distribution</li>\n",
    "<li>Most methods are ased on linear assumptions</li>\n",
    "<li>Most ML requires data to be standard normally distributed. Gaussian with 0 mean and unit variance</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa2d74-68c7-420a-b542-eeb9401860a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Standardization /Mean removal / Variance Scaling ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67971e0f-614c-4b88-b478-a50d5c3ba28d",
   "metadata": {},
   "source": [
    "$$ X'= \\frac{(X-\\bar{X})}{\\sigma} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1749f70-b8fb-40cc-bde2-73c2b00cbf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X=Boston\n",
    "Y=Boston['MEDV']\n",
    "X_train, X_test,Y_train,Y_test=train_test_split(X,Y,\n",
    "                                                test_size=0.3, random_state=42)\n",
    "X_scaled=preprocessing.scale(X_train)\n",
    "X_scaled.mean(axis=0)\n",
    "#X_scaled.std(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5d5bc-38d6-4e6b-a528-8a8683a9d39a",
   "metadata": {},
   "source": [
    "Training data is scaled, we must do likewise with Test data. However the assumption is that mean and variance are the same between Test and Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b0985-6344-4ca2-b845-aa08fc00be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#StandardScaler\n",
    "scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "#scaler.scale_\n",
    "scaler.transform(X_train)\n",
    "scaler.mean_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ea9f1-deb3-4cbf-b40d-caf53d698e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(X_train)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987e34b-774b-4b4a-9c11-186908088f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(X_test)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(X_test)\n",
    "plt.show()\n",
    "scaler.mean_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285893d2-1bbc-441e-be73-67d5f67e88f1",
   "metadata": {},
   "source": [
    "## Min-Max Scaling Feature to a range ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a84833-887d-48f1-b195-0bbff0264809",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array([[1,-1,2],[2,0,0],[0,1,-1]])\n",
    "minmax_scaler=preprocessing.MinMaxScaler()\n",
    "X_train_minmax=minmax_scaler.fit_transform(X_train)\n",
    "X_train_minmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7037cc98-7ee8-47b5-9f60-f89cfbe7bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.array([[-3,-1,0],[2,1.5,4]])\n",
    "X_test_minmax=minmax_scaler.transform(X_test)\n",
    "X_test_minmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4bffc-787e-4aa9-9240-7ca8f8475f39",
   "metadata": {},
   "source": [
    "## Scaling Sparse data ##\n",
    "centering sparse data would destroy the sparseness structure in the data, and thus, rarely is a sensible thing to do.\n",
    "However, it can make sense to scale sparse inputs, especially when features are on a different scales.\n",
    "MaxAbsScaler and maxabs_scale were especially designed for scalling sparse data.\n",
    "\n",
    "## Scaling Vs Whitening ##\n",
    "it is sometimes not enough to to center and scale the features independently, since a downstream model can further make some assumption on the linear independance of the features.\n",
    "To adress this issue u can use sklearn.decomposition.PCA or sklearn.decomposition.RandomizedPCA with whiten=True to further remove the linear correlation across features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007e605-bc85-49a0-abe5-2d2adb0b37e9",
   "metadata": {},
   "source": [
    "# Normalization #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa723cf-83e5-4b02-9091-6e4684085a02",
   "metadata": {},
   "source": [
    "Normalization is the process of scaling individual samplesto have unit form.\n",
    "This process can be useful if u plan to use quadratic form such as dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
    "$$ X'= \\frac{X-X_{mean}}{X_{max}-X_{min}} $$\n",
    "<p>\n",
    "This assumption is the base of the vector space model often used in text classification and culstering contexts.\n",
    "there are 2 types of Normalization:\n",
    "<ol>\n",
    "<li> <b>L1 Normalization:</b> least absolute deviations ensure the sum of absolute values is 1 in each row. </li>\n",
    "<li> <b>L2 Normalization:</b> least squares, ensure that the sum of squares is 1. </li>\n",
    "</ol>\n",
    "</p>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b32901-5a4b-4c88-bb81-52b3d9b65a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X=[[1,-1,2], [2,0,0],[0,1,-1]]\n",
    "X_norm=preprocessing.normalize(X,norm='l2')\n",
    "X_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132aebb-d3c9-4c39-8262-2b82d3d8e81a",
   "metadata": {},
   "source": [
    "The preprocessing module further provides a utility class \"Normalizer\" that implements the same operation usig the \"Transformer\" API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a10b4-a806-40f2-8cc1-4bc9eb63e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer=preprocessing.Normalizer()#.fit(X)\n",
    "normalizer\n",
    "normalizer.transform(X)\n",
    "#normalizer.transform([[-1,1,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865d82e-0496-468d-ac1c-06c3d32c8701",
   "metadata": {},
   "source": [
    "# Binarization #\n",
    "$$ f(x) = 0,1 $$\n",
    "feature binarization is the process of thresholding numerical features to get boolean values.This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi variate Bernouilli distribution .\n",
    "It is also common among the text processing community to use binary feature values(probably to simplify the probabilistic reasoning) even if normalized counts (term frequencies) or TF-IDF valued features often perform slightly better in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce21a9-2fe2-4b71-b214-d19ffa86283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[[1,-1,2], [2,0,0],[0,1,-1]]\n",
    "binarizer= preprocessing.Binarizer()#.fit(X)\n",
    "binarizer.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f92dfa5-1b40-4c87-84d4-982c8a500562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifying the threshold, threshold: if x<threshold: x=0\n",
    "binarizer= preprocessing.Binarizer(threshold=-0.5)\n",
    "binarizer.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd8330-30ef-4c9b-be79-10737b571404",
   "metadata": {},
   "source": [
    "### categorical features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f0da2-1240-407c-8585-3424e6e42956",
   "metadata": {},
   "outputs": [],
   "source": [
    "source=['Australia', 'Singapore', 'New Zealand', 'Hong Kong']\n",
    "label_enc=preprocessing.LabelEncoder()\n",
    "src=label_enc.fit_transform(source)\n",
    "for k,v in enumerate (label_enc.classes_):\n",
    "    print(f'{v}:\\t{k}')\n",
    "test_data=['Hong Kong', 'Singapore', 'Australia', 'New Zealand']\n",
    "result=label_enc.transform(test_data)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ee39c-05de-49a0-b020-47bcdfc61ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_enc=OneHotEncoder(sparse_output=False,categories='auto')\n",
    "src=src.reshape(len(src),1)\n",
    "one_hot=one_hot_enc.fit_transform(src)\n",
    "one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44edcaa-5176-412a-aa5d-95fbfa8df76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_res=label_enc.inverse_transform([np.argmax(one_hot[0,:])])\n",
    "invert_res\n",
    "invert_res=label_enc.inverse_transform([np.argmax(one_hot[2,:])])\n",
    "invert_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb24ca8-4587-418b-bc59-7d1dc6beb967",
   "metadata": {},
   "source": [
    "# Variance Bias Trade off #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7250a0-7559-4f2f-b646-30de4dd87e23",
   "metadata": {},
   "source": [
    "Every estimator has its advantages and its drawbacks. Its generalization error can be decomposed in terms of Bias, variance and noise.\n",
    "The bias of an estimator is its average error for different training sets. The variance of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the datasets. \n",
    "Bias and Noise are inherent properties of estimators and we usually have to select learning algorithms and hyperparameters so that both bias and variance are as low as possible.\n",
    "Another way to reduce the variance of a model is to use more training data. However, you should only collect more training data if the true function is too complex to be approximated by an estimator with a lower variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a4c6f-99e3-4f18-9194-25308725e804",
   "metadata": {},
   "source": [
    "## Validation Curve ##\n",
    "<ul>\n",
    "<li>for identifying over and under fitting.</li>\n",
    "<li>for plotting training and validation scores VS model parameters.</li>\n",
    "</ul> \n",
    "\n",
    " ### a/ For ridge regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9bf01-248e-431d-b4a7-ba4dd421e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "np.random.seed(0)\n",
    "iris=pd.read_csv('Data\\Iris.csv',delim_whitespace=False, header='infer')\n",
    "iris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb3717-3fc5-4b6b-b3bf-332a57bc4aae",
   "metadata": {},
   "source": [
    " ### b/ Plotting Validation Curve ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f31db3-b37c-4d9a-abc1-58c6fb8dfa8c",
   "metadata": {},
   "source": [
    "Training scores and validation scores of a Supported Vector Model for different values of the kernel parameter gamma. For very low values of gamma, u can see that both the training score and the validation score are low: this is called underfitting.<br>\n",
    "Medium values of gamma will result in high values for both scores.<br>\n",
    "If gamma is too high, the classifier will overfit, which means that the training score is good but the validation score is poor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56047a29-bd68-4461-867c-a3fc8a664b39",
   "metadata": {},
   "source": [
    " ### c/ Learning Curve ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a22a8b-b18f-4d61-bb0a-ac6e4b99f80a",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li> Shows the validation and training score of of an estimator for varying numbers of training samples.</li>\n",
    "<li> A tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error.</li>\n",
    "<li> if both the validation score and the training score converge to a value that is too low with increasing size of the training set, we will not benefit much from more training data.</li>\n",
    "</ul>\n",
    "### The high bias learning curve (fig1) ###\n",
    "<ul>\n",
    "<li> Low training and test accuracy /score , underfits the training data\n",
    "<li> The actual performance (accuracy or R²) level is far from the desired level of performance \n",
    "</ul>\n",
    "address by:\n",
    "<ul>\n",
    "<li> Increase the number of parameters, adding or creating new feature. </li>\n",
    "<li> Decrese the regularisation. </li>\n",
    "</ul>\n",
    "### The high variance learning curve (fig2) ###\n",
    "<ul>\n",
    "<li> The training and test level de not converge or converge slowly . </li>\n",
    "<li> The training and test level are still very far apart, this is an example of overfitting  . </li>\n",
    "</ul>\n",
    "address by:\n",
    "<ul>\n",
    "<li> Collect more training data </li>\n",
    "<li> Reduce model complexity </li>\n",
    "<li> Increase regularisation </li>\n",
    "</ul>\n",
    "### An example (fig3)of good variance bias trade off learning curve. ###\n",
    "<ul>\n",
    "<li> The actual level of performance (accuracy or R²) achieved is close to desired level of performance.</li>\n",
    "<li> The training and test (validation curve) are tight and converge to similar level.</li>\n",
    "</ul>\n",
    "<img src=\"Img_1.jpg\" alt=\"variance bias trade off\" width=\"600\"/>\n",
    "\n",
    "in summary, we look for :\n",
    "<ul>\n",
    "<li> <b>Bias:</b> Evaluate via score /accuracy level. </li>\n",
    "<li> <b> Variance: </b>Evaluate via the convergence speed and distance between trainig score/accuracy level and test score/accuracy level. </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50d0f3-3337-4ec6-b11b-3c6c42fd89cd",
   "metadata": {},
   "source": [
    "www.statology.org Explanation\n",
    "\n",
    "\n",
    "What is the Bias-Variance Tradeoff in Machine Learning?\n",
    "\n",
    "To evaluate the performance of a model on a dataset, we need to measure how well the model predictions match the observed data.\n",
    "\n",
    "For regression models, the most commonly used metric is the mean squared error (MSE), which is calculated as:\n",
    "\n",
    "MSE = (1/n)*Σ(yi – f(xi))2\n",
    "\n",
    "where:\n",
    "\n",
    "    n: Total number of observations\n",
    "    yi: The response value of the ith observation\n",
    "    f(xi): The predicted response value of the ith observation\n",
    "\n",
    "The closer the model predictions are to the observations, the smaller the MSE will be.\n",
    "\n",
    "However, we only care about test MSE – the MSE when our model is applied to unseen data. This is because we only care about how the model will perform on unseen data, not existing data.\n",
    "\n",
    "For example, it’s nice if a model that predicts stock market prices has a low MSE on historical data, but we really want to be able to use the model to accurately forecast future data.\n",
    "\n",
    "It turns out that the test MSE can always be decomposed into two parts:\n",
    "\n",
    "(1) The variance: Refers to the amount by which our function f would change if we estimated it using a different training set.\n",
    "\n",
    "(2) The bias: Refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.\n",
    "\n",
    "Written in mathematical terms:\n",
    "\n",
    "Test MSE = Var(f̂(x0)) + [Bias(f̂(x0))]2 + Var(ε)\n",
    "\n",
    "Test MSE = Variance + Bias2 + Irreducible error\n",
    "\n",
    "The third term, the irreducible error, is the error that cannot be reduced by any model simply because there always exists some noise in the relationship between the set of explanatory variables and the response variable.\n",
    "\n",
    "Models that have high bias tend to have low variance. For example, linear regression models tend to have high bias (assumes a simple linear relationship between explanatory variables and response variable) and low variance (model estimates won’t change much from one sample to the next).\n",
    "\n",
    "However, models that have low bias tend to have high variance. For example, complex non-linear models tend to have low bias (does not assume a certain relationship between explanatory variables and response variable) with high variance (model estimates can change a lot from one training sample to the next).\n",
    "The Bias-Variance Tradeoff\n",
    "\n",
    "The bias-variance tradeoff refers to the tradeoff that takes place when we choose to lower bias which typically increases variance, or lower variance which typically increases bias.\n",
    "\n",
    "The following chart offers a way to visualize this tradeoff:\n",
    "\n",
    "<img src='\\\\Img_3.png' alt='error with model complexity' width=800 />\n",
    "\n",
    "The total error decreases as the complexity of a model increases but only up to a certain point. Past a certain point, variance begins to increase and total error also begins to increase.\n",
    "\n",
    "In practice, we only care about minimizing the total error of a model, not necessarily minimizing the variance or bias. It turns out that the way to minimize the total error is to strike the right balance between variance and bias.\n",
    "\n",
    "In other words, we want a model that is complex enough to capture the true relationship between the explanatory variables and the response variable, but not overly complex such that it finds patterns that don’t really exist.\n",
    "\n",
    "When a model is too complex, it overfits the data. This happens because it works too hard to find patterns in the training data that are just caused by random chance. This type of model is likely to perform poorly on unseen data.\n",
    "\n",
    "But when a model is too simple, it underfits the data. This happens because it assumes the true relationship between the explanatory variables and the response variable is more simple than it actually is.\n",
    "\n",
    "The way to pick optimal models in machine learning is to strike the balance between bias and variance such that we can minimize the test error of the model on future unseen data.\n",
    "\n",
    "In practice, the most common way to minimize test MSE is to use cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c58ebf6-7235-4fcc-ba90-f16cbb25be73",
   "metadata": {},
   "source": [
    "### d/ Validation Curve ###\n",
    "By looking at the curve, we can determine if the model is underfitting, overfitting or just-right for some range of hyperparameter values of max_depth. Note that, in the graph, the accuracy score of the train set is marked as the “Training Score” and the accuracy score of the test set is marked as the “Cross-Validation Score”.\n",
    "<ul>\n",
    "<li><b>Underfitting: </b>Accuracy scores of both train and test sets are low. This indicates that the model is too simple or has been regularized too much. At the max_depth values of 1 and 2, the random forests model is underfitting.</li>\n",
    "<li><b>Overfitting: </b>The training accuracy score is very high and the accuracy score of the test set is low. The model fits very well for the training data, but it fails to generalize to new input data. For max_depth values of 4, 5, …, 10, the model is highly overfitted.</li>\n",
    "<li><b>Just-right: </b>No overfitting or underfitting. At the max_depth value of 3, the model is just right. The model fits the training data very well and it is also generalizable to new input data. That’s what we want!</li>\n",
    "</ul>\n",
    "Be careful: When you use an evaluation metric such as MSE, the overfitting condition happens when the training MSE is very low (not high) and the MSE of the test set is high (not low). This is because here we consider an error (Mean Squared Error).\n",
    "\n",
    "Be careful: Here, you got the optimal max_depth hyperparameter value of 3. Keep in mind that this is what we got when we consider only the max_depth hyperparameter. When we consider several hyperparameters at a time as in Grid Search or Randomized Search, the optimal max_depth hyperparameter value will not be 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8b131-a4a4-42bf-8968-505e698b5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_learning_curve(estimator, title, X_train,y_train,ylim=None,cv=None, n_jobs=1, \n",
    "                       train_sizes=np.linspace(0.1,1,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa73a75-4542-4901-9653-cc728520ae20",
   "metadata": {},
   "source": [
    "# Cross validation #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485c071-098d-4985-a6e5-53bd1378113f",
   "metadata": {},
   "source": [
    "<ul><li>hold out cross validation</li>\n",
    "<li>k-fold cross validation</li></ul>\n",
    "A test set should still be held out for final validation, but the vlidation set is no longer needed when doing CV.\n",
    "In the basic approach called k-fold CV, the training set is split into k smaller sets.The follwing procedure is followed for each of the k \"folds\".\n",
    "<ul><li>A model is trained using k-1 of the folds as training data.</li>\n",
    "<li>The resulting model is validated on the remaining part of the data (it is used as a test set to compute a performance mesure such as accuracy)</li></ul>\n",
    "The performance mesure reported by k-fold CV is then the average of the values computed in the loop.\n",
    "\n",
    "## Holdout method ##\n",
    "<ul>\n",
    "<li>Split initial dataset into a separate training and test dataset </li>\n",
    "<li>Training dataset - model training</li>\n",
    "<li>Test dataset - estimate its generalisation performance</li>\n",
    "</ul>\n",
    "A variation is to split the training set to 2: training set and validation set\n",
    "<ul>\n",
    "<li><b>Training set : </b>for fitting different models </li>\n",
    "<li><b>Validation set: </b>for tuning and comparing different parameter settings to further improve the performance for making predictions on unseen data.And finally for model selection. </li>\n",
    "</ul>\n",
    "This process is called model selection, we want to select the optimal values of tuning parameters (hyperparameters)\n",
    "\n",
    "## k-fold Cross Validation ##\n",
    "<ul>\n",
    "<li>Randomly split the training dataset into k folds without replacement.\n",
    "<li>k-1 folds are used for the model training.\n",
    "<li>The one fold is used for performance validation.\n",
    "</ul>\n",
    "This procedure is repeated k times.\n",
    "Final outcomes: k models and performance estimates \n",
    "<ul>\n",
    "<li>Calculate the average performance of the models based on the different, independant folds to obtain a performance estimate that is less sensitive to the sub partitioning of the training data compared to the holdout method.</li>\n",
    "<li>k fold CV is used for model tuning.Finding the optimal hyperparameter values that yields a satisfying generalization performance.</li>\n",
    "<li>Once we have found satisfactory hyperparameter values, we can retrain the model on the complete training set and obtain a final performance estimate using the independant test set.\n",
    "The rationale behind fitting a model to the whole training dataset after k fold CV is that proving more training samples to a learning algorithm usually results in a more accurate and robust model.</li>\n",
    "<li>common k is 10 </li>\n",
    "<li>For relatively small training set, increase the number of folds </li>\n",
    "</ul>\n",
    "\n",
    "## Stratified k-fold Cross Validation ##\n",
    "<ul>\n",
    "<li>variation of K fold</li>\n",
    "<li>Can yield better bias and variance estimates, especially in cases of unequal class proportions</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79f2cdb-bd0d-47f7-8308-e74f87128acc",
   "metadata": {},
   "source": [
    "# Cross Validation Illustration #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8f901-4296-4819-a6c0-e1b9ce9cfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "Boston_df=Boston.iloc[:,0:13]\n",
    "target=Boston['MEDV']#.values\n",
    "Boston_df.shape\n",
    "\n",
    "X_train,X_test,Y_train, Y_test=train_test_split(Boston_df,target,\n",
    "                                                test_size=0.4,random_state=0)\n",
    "print(f'X_train,X_test,Y_train, Y_test shapes:{X_train.shape}, {X_test.shape},{Y_train.shape},{Y_test.shape}')\n",
    "\n",
    "regression=svm.SVR(kernel='linear', C=1).fit(X_train, Y_train)\n",
    "regression.score(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644acf0e-c94d-4bf3-a7f2-e12a8500a742",
   "metadata": {},
   "source": [
    "## Computing Cross-Validated metrics ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bd993-7dc2-4843-9990-5139bb15a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "regression=svm.SVR(kernel='linear', C=1)\n",
    "scores=cross_val_score(regression, Boston_df, target, cv=5) #cv is k folds\n",
    "\n",
    "print(f'Accuracy : %.3f +/- %.3f'% (scores.mean(), scores.std()**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba7f60c-ea79-4cc8-91ff-b90045b1937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use MSE\n",
    "scores=cross_val_score(regression, Boston_df, target, cv=5,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "\n",
    "print(f'Accuracy : %.3f +/- %.3f'% (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0074467-5150-4b1a-a0ca-6da7b7daead1",
   "metadata": {},
   "source": [
    "### K-fold ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f15b3-3bc8-42cb-8032-1a78a384a7c7",
   "metadata": {},
   "source": [
    "it divides all the samples in k group of samples called folds (if k=n, this is equivalent to the leave one out strategy), <br>of equal sizes(if possible). The prediction function is learned using k-1 folds, and the fold left out is used for test.<br>\n",
    "This is an example of 2 fold CV on a dataset with 4 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d15b6b-0d0f-4c4d-958d-5b1ce38dd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X= ['a', 'b', 'c', 'd']\n",
    "kf= KFold(n_splits=2)\n",
    "for train, test in kf.split(X):\n",
    "    print(f'%s %s'% (train, test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae85a66-511d-4dcf-8783-431d0102b8ad",
   "metadata": {},
   "source": [
    "### Stratified K-fold ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b48856-d140-4b31-b51a-4a635a65fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X=np.ones(10)\n",
    "Y= [0,0,0,0,1,1,1,1,1,1,]\n",
    "skf=StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X,Y):\n",
    "    print(f'%s %s'% (train, test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58079026-e55b-4872-9503-59bdf1f8066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe_svm=make_pipeline(StandardScaler(), PCA(n_components=2), svm.SVR(kernel='linear', C=1))\n",
    "pipe_svm.fit(X_train,Y_train)\n",
    "y_pred=pipe_svm.predict(X_test)\n",
    "print(f'Test Accuracy= %.3f' % pipe_svm.score(X_test,Y_test))\n",
    "#print(f'Test Accuracy= {pipe_svm.score(X_test,Y_test):.3f}' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e08d1a-bdfa-4ac1-afa1-4a21f9ffc42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd method\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores= cross_val_score(estimator=pipe_svm,X=X_train, y=Y_train, cv=10,n_jobs=1)\n",
    "print(f'CV Accuracy= %s' % scores)\n",
    "print(f'CV accuracy= %.3f +/- %.3f'% (scores.mean(), scores.std()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
