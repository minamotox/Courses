{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f059a-df16-447d-8c9d-72a3761f5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f504be-afc4-48c1-80f9-17ed40dd5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.executable #sys.version_info\n",
    "sys.version\n",
    "print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98904c81-78c7-4d94-9534-e8d211c1c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "\n",
    "print('python:',sys.version)\n",
    "print('Numpy:',np.__version__)\n",
    "print('Pandas:',pd.__version__)\n",
    "print('Scikitlearn:',sklearn.__version__)\n",
    "print('Seaborn: ',sns.__version__)\n",
    "print('matplotlib:',matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8d75a-4920-410f-82ce-b875f25172ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Boston=pd.read_csv('Data/housing.data',delim_whitespace=True, header=None)\n",
    "Boston\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c3db8-88dd-47a8-b157-77e22fb826e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table style=\"width:60%\">\n",
    "<tr>\n",
    "<th> Code :   </th>\n",
    "<th> Description : </th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th> CRIM </th> \n",
    "<th> per capita crime rate per town  </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th> ZN </th>\n",
    "<th> proportion of residential land zones for lots over 25000 sgr ft ></th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th> INDUS </th>\n",
    "<th> proportion of non retail business acr per town </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th> CHAS</th>\n",
    "<th> Charles River dummy variable (=1 if tract bounds river) </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th> NOX </th>\n",
    "<th> NO concentration (pp 10 million) </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th> RM </th>\n",
    "<th> Average N rooms  </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>  AGE  </th>\n",
    "<th> proportion of owner occupied builts prior 1940 </th>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>  DIS  </th>\n",
    "<th> weighted distances to 5 Boston employment centers </th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th>  RAD  </th>\n",
    "<th> index of accessibility to radial highways </th>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<th>  TAX  </th>\n",
    "<th> full value property tax rate per 10 000 $ </th>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<th>  PTRATIO </th>\n",
    "<th> pupil teacher ratio by town </th>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<th> B </th>\n",
    "<th> 1000(BK-0.63)² ; bk proportion of blacks by town </th>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<th> LSTAT </th>\n",
    "<th> % lower status of the population </th>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<th>  MEDV  </th>\n",
    "<th> median value of owner occupied homes in 1000 $ </th>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29831956-d40e-47a9-9065-8313c387f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boston.columns=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX',\n",
    "                'PTRATIO','B','LSTAT','MEDV']\n",
    "#Boston.to_csv('Boston.csv')\n",
    "Boston.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b0bd3-6d27-4256-9082-e3d3ee7838de",
   "metadata": {},
   "source": [
    "# Exploratory data Analysis (EDA) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8cf5f-873c-4cf3-8ac0-d8a165b77792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.pairplot(Boston, height=1.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a02af-bd87-4704-9798-3b3d91d3fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Features=['CRIM','ZN','INDUS','NOX','RM']\n",
    "sns.pairplot(Boston[Features], height=2.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044e487-caaa-4f7d-945a-4a230cb51777",
   "metadata": {},
   "outputs": [],
   "source": [
    "Features2=['AGE','TAX','PTRATIO','B','LSTAT','MEDV']\n",
    "sns.pairplot(Boston[Features2], height=2.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0647e50-cbb4-4c16-8d7c-24e0069f653a",
   "metadata": {},
   "source": [
    "# Correlation & Features selection #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af57ec-065d-4909-854f-9f16d42f4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format='{:,.3f}'.format\n",
    "Boston.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8037da-a400-4312-8ac9-71a2334e1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.heatmap(Boston.corr(), annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64f13e-2dcd-426f-9a6c-fba2d378ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(Boston[['CRIM','ZN','INDUS','CHAS','MEDV']].corr(), annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b07f55-7b1e-42f9-adb4-b8f2ced365ae",
   "metadata": {},
   "source": [
    "# Linear regression with Scikit-Learn #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa4019-8c3b-4228-9f55-ef3c4ccc0f95",
   "metadata": {},
   "source": [
    "## Five steps of in using Scikit-earn estimator API by Jacob T. Vanderplas: ##\n",
    "<ol>\n",
    "<li> Choose class model by importing the appropriate estimator class from Scikit-earn </li>\n",
    "<li> Choose model hyperparameters by instanciating this class with desired values\n",
    "<li> Arrange data into a features matrix and target vector\n",
    "<li> Fit the model to ur data by calling the fit() method of the model instance\n",
    "<li> Apply the model to the new data:\n",
    "<ul>\n",
    "<li> for supervised learning: often we predict labels for unknown data using predict() method\n",
    "<li> for unsupervised learning: often we transform or infer properties of the data using the transform() or predict() method\n",
    "</ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2cd88-87ab-40d7-87fb-44b34e92f58b",
   "metadata": {},
   "source": [
    "### MEDV vs RM Linear regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566208fc-dc41-4ec9-9e5b-4a095544e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X=Boston['RM'].values.reshape(-1,1)\n",
    "Y=Boston['MEDV'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74699da-0572-46d9-b171-67089b0c2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LinearRegression()\n",
    "model.fit(X,Y)\n",
    "print(f'linear model coeficient={model.coef_} and B={model.intercept_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f0299-cfaf-44ae-821d-030d045c1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x=X,y=Y,color='orange')\n",
    "plt.xlabel('Average N of rooms')\n",
    "plt.ylabel('owner occupied houses mediane value 1000$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace6087-fe84-44ef-a9e9-0fa6ca5d6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='RM', y='MEDV', data=Boston, kind='reg',height=10,color='orange')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6fc2e-d0bf-4418-adf2-8f4841654de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.predict(np.array([5]).reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8dcbdf-0a4e-420a-b05f-5f072ec39d3f",
   "metadata": {},
   "source": [
    "### LSTAT vs MEDV Linear regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491c603-415b-420f-82b3-edb9a3d8d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X2=Boston['LSTAT'].values.reshape(-1,1)\n",
    "Y2=Boston['MEDV'].values\n",
    "model2=LinearRegression()\n",
    "model2.fit(X2,Y2)\n",
    "print(f'linear, a={model2.coef_} and b={model2.intercept_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168321fb-54b3-4445-934d-3ef79bc28beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x=X2,y=Y2,color='DeepSkyBlue');\n",
    "plt.xlabel('Median value of owner occupied homes in 1000 $')\n",
    "plt.ylabel('% lower status of the population')\n",
    "plt.grid()\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55977980-40b5-49da-90ff-1b07f55d6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional\n",
    "sns.jointplot(x='LSTAT', y='MEDV', data=Boston, kind='reg',height=10,color='lime')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c29740-923b-4459-8dce-15f5b29a7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.predict(np.array([15]).reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee42e92-41c6-4036-b926-a42f4bbd5831",
   "metadata": {},
   "source": [
    "# Robust Regression #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3fd359-3138-40f8-9dff-d1ce1aefb8b8",
   "metadata": {},
   "source": [
    "## Random Sample Consensus (RANSAC) ##\n",
    "<ol>\n",
    "<li> select <strong> min_samples </strong>random samples from original data and check whether the dataset is valid <strong>(is_data_valid)</strong> </li>\n",
    "<li> fit a model to a random subset <strong>(base_estimator.fit)</strong> and check whether the estimated model is valid <strong> (is_model_valid)</strong> </li>\n",
    "<li> Classify all data as inliers or ouliers by calculating the residuals to the estimated model <strong>(base_estimator.predict(x)-y)</strong> all data samples\n",
    "with absolute residuals smaller than the <strong>residual_threshold </strong>are considered as inliers </li>\n",
    "<li> save fitted model as best model if number of outliers samples is maximal, in case the current estimated model has the same number of inliers,\n",
    "    it is considered as the best model if it has better score</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec65a32-9d00-4cca-bb5d-c246c99ce1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "#RM vs MEDV\n",
    "#X=Boston['RM'].values.reshape(-1,1)\n",
    "#Y=Boston['MEDV'].values\n",
    "coef=True\n",
    "# Robustly fit linear model with RANSAC algorithm\n",
    "ransac = linear_model.RANSACRegressor()\n",
    "ransac.fit(X,Y)\n",
    "inlier_mask = ransac.inlier_mask_\n",
    "outlier_mask = np.logical_not(inlier_mask)\n",
    "\n",
    "# Compare estimated coefficients\n",
    "print(f'Estimated coefficients (coef:{coef} , linear regression: {model.coef_},RANSAC= {ransac.estimator_.coef_}x+{ransac.estimator_.intercept_})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2a484-fd62-4793-8fb9-af9c282062f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions and plotting\n",
    "line_x=np.arange(3,10,1)\n",
    "line_y_ransac=ransac.predict(line_x.reshape(-1,1))\n",
    "line_y=model.predict(line_x.reshape(-1,1))\n",
    "\n",
    "sns.set(style='darkgrid', context='notebook')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[inlier_mask], Y[inlier_mask], c='blue', marker='o', label='Inliers')\n",
    "plt.scatter(X[outlier_mask], Y[outlier_mask], c='violet', marker='s', label='Outliers')\n",
    "plt.plot(line_x, line_y, color='grey',linewidth=2, label=\"Linear regressor\")\n",
    "plt.plot(line_x,line_y_ransac,color=\"red\", linewidth=2,label=\"RANSAC regressor\")\n",
    "plt.xlabel('Average N rooms')\n",
    "plt.ylabel('median value of owner occupied homes in 1000 $ ')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13b4df-ccd3-4f80-856a-c64d943c5872",
   "metadata": {},
   "source": [
    "**linear regression Assumptions:** https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2889f14b-e164-4bdb-9620-10514469b9ed",
   "metadata": {},
   "source": [
    "# Evaluate Regression Model Performance #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcfea6e-2001-4357-80ed-4935611ff9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=Boston.iloc[:, :-1].values\n",
    "Y=Boston['MEDV'].values\n",
    "\n",
    "x_train, x_test,y_train, y_test=train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "lr=LinearRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "y_train_pred=lr.predict(x_train)\n",
    "y_test_pred=lr.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b42596-7f9b-48c0-86a6-0ec6971f1db3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Method1: Residual Analysis ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40fa90-984b-4d49-bdc5-e59568c0d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(y_train_pred, y_train_pred-y_train, c='blue', marker='s', label='Training')\n",
    "plt.scatter(y_test_pred,y_test_pred-y_test, c='orange', marker='o', label='Test')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals ')\n",
    "plt.legend(loc='upper right')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='k')\n",
    "plt.xlim([-10,50])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd0362-667a-4ede-aed0-82dc69b065e1",
   "metadata": {},
   "source": [
    "### Method2: Mean Squarred Error (MSE) ###\n",
    "<ul>\n",
    "<li>The average value of the sums of the squarred error cost function </li>\n",
    "<li>Useful for comparing different regression models </li>\n",
    "<li>For tuning parameters via a grid search and cross-validation </li>\n",
    "</ul>\n",
    "\n",
    "## $$MSE={{1}\\over{n }}{\\sum_{i=1}^n{(yi-ŷi)²}}$$ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61066b3d-c5e9-49ca-9e39-77da3cae0523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_train,y_train_pred)\n",
    "mean_squared_error(y_test,y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e5ecdf-8a20-4847-8ded-6de7c6fcb6c0",
   "metadata": {},
   "source": [
    "### Method3: Coefficient of determination R² ###\n",
    "### $$ R²=1-{SSE \\over SST}$$ ###\n",
    "<ul>\n",
    "<li>SSE: Sum of squarred errors </li>\n",
    "<li>SST: Total sum of squares </li>\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2133c8-716a-49f9-bf1b-4bbe67079112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_train,y_train_pred)\n",
    "r2_score(y_test,y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa7ea84-3ec7-438c-9053-edf6e670bb00",
   "metadata": {},
   "source": [
    "# The perfect model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b71ffc-ffe5-4a0f-89b1-18bebe3594da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "generate_random=np.random.RandomState(0)\n",
    "x=10*generate_random.rand(1000)\n",
    "y=3*x+np.random.randn(1000)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(x,y, marker='^', c='indigo')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f9cad-8a19-4cfa-87a4-04df9e353d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test,y_train, y_test=train_test_split(x,y, test_size=0.3, random_state=0)\n",
    "model=LinearRegression()\n",
    "model.fit(x_train.reshape(-1,1),y_train)\n",
    "y_train_pred=model.predict(x_train.reshape(-1,1))\n",
    "y_test_pred=model.predict(x_test.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718bb02-92be-4049-b88c-e912f198309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual Analysis\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(y_train_pred, y_train_pred-y_train, c='blue', marker='s', label='Training')\n",
    "plt.scatter(y_test_pred,y_test_pred-y_test, c='orange', marker='o', label='Test')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals ')\n",
    "plt.legend(loc='upper right')\n",
    "plt.hlines(y=0, xmin=3, xmax=33, lw=2, color='r')\n",
    "plt.xlim([-5,35])\n",
    "plt.ylim([-25,15])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14a5b4-033d-49fb-be61-65589ad6b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_train,y_train_pred)\n",
    "mean_squared_error(y_test,y_test_pred)\n",
    "print(f'MSE train:{mean_squared_error(y_train,y_train_pred)} , MSE Test:{mean_squared_error(y_test,y_test_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ee497-5fcd-493d-b7c5-161eb49fc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R²\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_train,y_train_pred)\n",
    "r2_score(y_test,y_test_pred)\n",
    "print(f'R² train:{mean_squared_error(y_train,y_train_pred)} , R² Test:{mean_squared_error(y_test,y_test_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cac56-3810-474f-bfb8-f222701495bd",
   "metadata": {},
   "source": [
    "# Multiple Regression #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295c83b-373c-428a-9be5-554aad3d0c43",
   "metadata": {},
   "source": [
    "## y=$\\beta$0+$\\beta1$*x1+$\\beta2$*x2... ##\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c4012-6106-4402-8d35-32c772133bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Housing=pd.read_csv('housing.data',delim_whitespace=True, header=None)\n",
    "Housing.columns=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX',\n",
    "                'PTRATIO','B','LSTAT','MEDV']\n",
    "Boston=Housing.iloc[:,0:13]\n",
    "\n",
    "X=Boston\n",
    "Y=Housing.iloc[:,13:15]\n",
    "print(f'x= \\n {X} and y=\\n {Y}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ecb1b-8ff9-454e-96ec-e6c1e94aeb6a",
   "metadata": {},
   "source": [
    "# Statsmodels #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95072027-dd11-4b94-8ec7-b39f95907153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_constant=sm.add_constant(X)\n",
    "pd.DataFrame(X_constant)\n",
    "#sm.OLS?\n",
    "model=sm.OLS(Y,X_constant)\n",
    "lr=model.fit()\n",
    "lr.summary()\n",
    "#P must be <0.025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2d685-13c9-4652-94a2-f72de44f4817",
   "metadata": {},
   "source": [
    "## Summary: ##\n",
    "<ul>\n",
    "<li> <b>coefficient:</b> it is the value of the intercept. For each variable, it is the measurement of how change in that variable affects the independent variable. It is the ‘m’ in ‘y = mx + b’ One unit of change in the dependent variable will affect the variable’s coefficient’s worth of change in the independent variable. </li>\n",
    "<li><b>std error:</b> is an estimate of the standard deviation of the coefficient, a measurement of the amount of variation in the coefficient throughout its data points. A low std error compared to a high coefficient produces a high t statistic, which signifies a high significance for your coefficient.</li>\n",
    "<li><b>P>|t|</b> is one of the most important statistics in the summary. It uses the t statistic to produce the p-value, a measurement of how likely your coefficient is measured through our model by chance. The p-value of 0.378 for Wealth is saying there is a 37.8% chance the Wealth variable has no affect on the dependent variable, Lottery, and our results are produced by chance. A common alpha is 0.05, which few of our variables pass in this instance.</li>\n",
    "<li><b>[0.025 and 0.975]</b> are both measurements of values of our coefficients within 95% of our data, or within two standard deviations. Outside of these values can generally be considered outliers.</li> </ul>\n",
    "The p-value is the smallest test size that would cause an observation of t=0.1 to lead to a rejection of the null hypothesis\n",
    "\n",
    "### Residual Tests: ###\n",
    "<ul>\n",
    "<li><b>Omnibus:</b> a combined statistic test for skewness and kurtosis</li>\n",
    "<li><b>prob(Omnibus):</b> P-value of Omnibus test</li>\n",
    "<li><b>Skewness:</b> a measure of symmetry of residuals around the mean.Zero if symmetrical. A positive value indicates a long tail to the right, a negative value indicates a long tail to the left</li>\n",
    "<li><b>Kurtosis:</b> A measure of the shape of distribution of the residuals, A normal distribution has 0 measure.A negative value points to a flatter than normal distribution, a positive one has a higher peak than normal distribution</li>\n",
    "<li><b>Durbin-Watson:</b> A test for presence of correlation among the residuals, this is important for time series modelling</li>\n",
    "<li><b>Jarque-Bera:</b> it is a combined statistical test of Skewness and Kurtosis</li>\n",
    "<li><b>Prob(JB): </b>p_value of Jarque-Bera</li>\n",
    "<li><b>Cond.No: </b>it is a test for multicolinearity. > 30 indicates unstable results.</li>\n",
    "    \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652a236-c896-4eac-80b0-678449d4067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "form_lr=smf.ols(formula='Y ~ CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT',data=Housing)\n",
    "mlr=form_lr.fit()\n",
    "mlr.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce5ee3-0939-4614-b8fa-2ad2f08aafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting 100 rows in basis of CRIM and Black\n",
    "model_ex=smf.ols(formula='Y ~ CRIM+B',data=Housing) #CRIM+ZN+CHAS+\n",
    "mlr_ex=model_ex.fit()\n",
    "mlr_ex.summary()\n",
    "#predictions = mlr_ex.predict(Housing[0:100])\n",
    "#predictions.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce83266-9490-4822-8fa7-b0f7407ee1b5",
   "metadata": {},
   "source": [
    "# Correlation Matrix #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac1229-dd2c-48cb-8cce-fcba1ebbcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Housing=pd.read_csv('Data/housing.data',delim_whitespace=True, header=None)\n",
    "Housing.columns=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX',\n",
    "                'PTRATIO','B','LSTAT','MEDV']\n",
    "Boston=Housing.iloc[:,0:13]\n",
    "X=Boston\n",
    "Y=Housing.iloc[:,13:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc0cfc-0329-4e70-b709-4ee69847aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format='{:,.2f}'.format\n",
    "corr_matrix=Boston.corr()\n",
    "corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e651ef9-c0f6-4738-845b-6aceff6704aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[np.abs(corr_matrix)< 0.6]=0 #lesser than 0.6 and greater than -0.6\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a393f19-f788-4500-bde0-24cd32dd0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette('tab20b',10) # Default color palette\n",
    "sns.palplot(palette) # Plotting your palette!\n",
    "#sns.palplot(sns.color_palette('husl', 20)) # Seaborn color palette, with number of colors \n",
    "#sns.color_palette('rocket', as_cmap=True) # Get a CMap\n",
    "\n",
    "plt.figure(figsize=(18,9))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=palette)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050732d-27b1-4bac-b05e-81a91ddb1b6f",
   "metadata": {},
   "source": [
    "# Detecting colinearity with Eigenvectors #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c1903-4ac8-41f5-b206-312545a48fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues , eigenvectors= np.linalg.eig(Boston.corr())\n",
    "pd.Series(eigenvalues).sort_values()\n",
    "np.abs(pd.Series(eigenvectors[:,8])).sort_values(ascending=False)\n",
    "print(Boston.columns[2],Boston.columns[8],Boston.columns[9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c75e12-f89c-4578-80e6-0d0bb8bb0658",
   "metadata": {},
   "source": [
    "small values= presence of colinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a07849-50e3-4992-b626-1bbae1ffb300",
   "metadata": {},
   "source": [
    "# Revising Feature importance and Extractions #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880e1e18-de08-492f-b17f-57f3c5a01635",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Boston['TAX'])\n",
    "#plt.hist(Boston['NOX'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ca0df-3819-4df8-9001-e6e24697a721",
   "metadata": {},
   "source": [
    "# Standardise variable to identify Key Features #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b11dc-75a0-4567-809c-f2543d982e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "pd.options.display.float_format='{:,.4f}'.format\n",
    "\n",
    "model=LinearRegression()\n",
    "model.fit(X,Y)\n",
    "mc=list(np.transpose(model.coef_))\n",
    "bc=list((Boston.columns))\n",
    "bcc=np.transpose(bc)\n",
    "result=pd.DataFrame({'Name':bc,'Coefficient':mc}).set_index('Name')\n",
    "co=np.abs(result).sort_values(by=['Coefficient'], ascending=False)\n",
    "co\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bec2e-7aac-4176-ae81-92adec0507ef",
   "metadata": {},
   "source": [
    "### 2nd method ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c25c8b-0eb8-4bfa-9e99-9e6630bfaac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "scaler=StandardScaler()\n",
    "standard_coefficient_linear_reg=make_pipeline(scaler,model)\n",
    "standard_coefficient_linear_reg.fit(X,Y)\n",
    "scl=list(np.transpose(standard_coefficient_linear_reg.steps[1][1].coef_))\n",
    "result=pd.DataFrame({'Name':bcc,'Coefficient':scl}).set_index('Name')\n",
    "co=np.abs(result).sort_values(by=['Coefficient'], ascending=False)\n",
    "co\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2525c38d-6851-40ea-b36e-7446e2d1f122",
   "metadata": {},
   "source": [
    "## Use R² to identify key features ##\n",
    "<ul>\n",
    "<li>Compare R² of the model with R² without a feature</li>\n",
    "<li>significant change in R² means the importance of the feature</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfca82a-7d36-4738-ae34-60b1b5383f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "linear_reg=smf.ols(formula='Y ~ CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT',data=Boston)\n",
    "Benchmark=linear_reg.fit()\n",
    "r2_score(Y,Benchmark.predict(Boston))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2910b-c56a-4f0c-8ae1-0caed7d5b687",
   "metadata": {},
   "source": [
    "## Without LSTAT ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712da64-1e91-4468-a99d-9e792a7525b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg=smf.ols(formula='Y ~ CRIM+ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B',data=Boston)\n",
    "lr_lstat=linear_reg.fit()\n",
    "r2_score(Y,lr_lstat.predict(Boston))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddefbeec-dca2-4fa8-b802-ee4c3f122ada",
   "metadata": {},
   "source": [
    "## Without AGE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e88f4b-f560-49e3-b2bb-cdaf121cef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg=smf.ols(formula='Y ~ CRIM+ZN+INDUS+CHAS+NOX+RM+DIS+RAD+TAX+PTRATIO+B+LSTAT',data=Boston)\n",
    "lr_AGE=linear_reg.fit()\n",
    "r2_score(Y,lr_AGE.predict(Boston))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0fb676-fdae-4d7a-8b94-759e4016c7a3",
   "metadata": {},
   "source": [
    "# Regularized Regression #\n",
    "<ul>\n",
    "<li>Ridge regression </li>\n",
    "<li>Least absolute shrinkage and selection operator(LASSO) </li>\n",
    "<li>Elastic net </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df4ae0-2bae-43c2-afc7-78acdfbf490b",
   "metadata": {},
   "source": [
    "## Ridge Regression ##\n",
    "\n",
    "<p>Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. \n",
    "This method performs <b>L2</b> regularization. When the issue of multicollinearity occurs, least-squares are unbiased, \n",
    "and variances are large, this results in predicted values being far away from the actual values. \n",
    "\n",
    "The cost function for ridge regression: </p>\n",
    "\n",
    "## $$ Min(||X( \\omega)-Y||_2^2 + \\lambda||\\omega||_2^2) $$ ##\n",
    "\n",
    "<p>Lambda is the penalty term. λ given here is denoted by an alpha parameter in the ridge function. So, by changing the values of alpha, we are controlling the penalty term. The higher the values of alpha, the bigger is the penalty and therefore the magnitude of coefficients is reduced.\n",
    "<ul>\n",
    "<li>   It shrinks the parameters. Therefore, it is used to prevent multicollinearity </li> \n",
    "<li>    It reduces the model complexity by coefficient shrinkage </li> \n",
    "<li> Check out the free course on regression analysis. </li> \n",
    "</ul>\n",
    "<b>Ridge Regression Models </b> \n",
    "\n",
    "For any type of regression machine learning model, the usual regression equation forms the base which is written as: </p>\n",
    "\n",
    "<div>\n",
    "$$ Y = X\\beta+e $$\n",
    "    </div>\n",
    "<p>\n",
    "Where Y is the dependent variable, X represents the independent variables, B is the regression coefficients to be estimated, and e represents the errors are residuals. \n",
    "Once we add the lambda function to this equation, the variance that is not evaluated by the general model is considered. After the data is ready and identified to be part of L2 regularization, there are steps that one can undertake.</p>\n",
    "<b>Standardization </b>\n",
    "<p>\n",
    "In ridge regression, the first step is to standardize the variables (both dependent and independent) by subtracting their means and dividing by their standard deviations. This causes a challenge in notation since we must somehow indicate whether the variables in a particular formula are standardized or not. As far as standardization is concerned, all ridge regression calculations are based on standardized variables. When the final regression coefficients are displayed, they are adjusted back into their original scale. However, the ridge trace is on a standardized scale.\n",
    "\n",
    "Also Read: Support Vector Regression in Machine Learning </p>\n",
    "<b>Bias and variance trade-off</b>\n",
    "\n",
    "Bias and variance trade-off is generally complicated when it comes to building ridge regression models on an actual dataset. However, following the general trend which one needs to remember is:\n",
    "\n",
    "    The bias increases as λ increases.\n",
    "    The variance decreases as λ increases.\n",
    "\n",
    "<b>Assumptions of Ridge Regressions</b>\n",
    "\n",
    "The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence. However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed.\n",
    "Now, let’s take an example of a linear regression problem and see how ridge regression if implemented, helps us to reduce the error.\n",
    "\n",
    "We shall consider a data set on Food restaurants trying to find the best combination of food items to improve their sales in a particular region. \n",
    "<p>\n",
    "if lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen. This technique works very well to avoid over-fitting issue.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec5205-2129-4df2-beae-b2bd1479bec1",
   "metadata": {},
   "source": [
    "## LASSO ##\n",
    "\n",
    "<p>Is a linear model that estimates sparse coefficients. Called L1 regularization.\n",
    "Mathematically, it consists in a linear model trained with $\\phi1$ prior as regularized.The objective function to minimize is:</p>\n",
    "\n",
    "## $$ Min_\\omega \\frac{1}{2n_{samples}}||X( \\omega)-Y||_2^2 + \\lambda||\\omega||_1 $$ ##\n",
    "<p> The LASSO estimate thus solves of least squares penalty with $\\lambda||\\omega||_1$ added, where $\\lambda$ is constant and $||\\omega||_1$ is the $\\phi1$ -norm of the parameter vector.</p>\n",
    "<p>\n",
    "The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a357087-205a-4501-b1b9-345e25b578fd",
   "metadata": {},
   "source": [
    "## ELASTIC Net ##\n",
    "\n",
    "<p>A linear regression model trained with L1 and L2 prior as regularizer.\n",
    "This combination allows for learning a sparse model where few of the weights are non 0 like LASSO, while still maintaining the regularization properties of ridge.\n",
    "<b>ELASTIC NET</b> is useful when there are multiple features which are correlated with one another.<b>LASSO</b> is likely to pick one of these randomly, while <b>ELASTIC NET</b> is likely to pick both.\n",
    "A practical advantage of trading-off between LASSO and Ridge is it allows <b>ELASTIC NET</b> to inherit some of Ridge's stability under rotation.\n",
    "the objective function to minimize is in this case:\n",
    "</p>\n",
    "\n",
    "## $$ Min_\\omega \\frac{1}{2n_{samples}}||X( \\omega)-Y||_2^2 + \\lambda p||\\omega||_1 + \\frac{\\lambda(1-p)}{2} ||\\omega||_2^2$$ ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87b875-af1e-41c7-beca-ee94dbbdcc30",
   "metadata": {},
   "source": [
    "# Outliers Impact #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611ecae-b5c7-4d16-aa21-b50c26d22cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766714be-13f1-4dec-8e44-48aecfd7fe03",
   "metadata": {},
   "source": [
    "## Linear regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2a4a4-6d3a-43be-9a0e-35df312d0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples=100\n",
    "rng=np.random.randn(n_samples)*10\n",
    "y_gen=0.5*rng+2*np.random.randn(n_samples)\n",
    "lr=LinearRegression()\n",
    "model=lr.fit(rng.reshape(-1,1), y_gen)\n",
    "model_pred=lr.predict(rng.reshape(-1,1))\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='b')\n",
    "plt.plot(rng,model_pred, color='r' )\n",
    "print(lr.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac2ae9-fc8a-49ab-9385-850a4340bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=rng.argmax()\n",
    "y_gen[idx]=200\n",
    "idx=rng.argmin()\n",
    "y_gen[idx]=-200\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='green')\n",
    "o_lr=LinearRegression()\n",
    "o_lr.fit(rng.reshape(-1,1), y_gen)\n",
    "o_model_predict=o_lr.predict(rng.reshape(-1,1))\n",
    "plt.scatter(rng, y_gen, color='grey')\n",
    "plt.plot(rng,o_model_predict, color='indigo' )\n",
    "print(o_lr.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03076e24-1894-41f3-bac5-aa0be451f4f0",
   "metadata": {},
   "source": [
    "## Ridge Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf0cc3-8664-4af3-8b07-182a95063b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#rng_N=preprocessing.normalize(rng.reshape(-1,1))\n",
    "ridge_mod=Ridge(alpha=0.5, fit_intercept=True)#, normalize=True)\n",
    "ridge_mod.fit(rng.reshape(-1,1), y_gen)\n",
    "ridge_mod_pred=ridge_mod.predict(rng.reshape(-1,1))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='red')\n",
    "plt.plot(rng,ridge_mod_pred, color='blue' )\n",
    "ridge_mod.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea26c4e-9176-4ee6-9a9c-b4f0804206da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge + normalize data ???\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge_modN = make_pipeline(StandardScaler(with_mean=False),RidgeCV())\n",
    "ridge_modN.fit(rng.reshape(-1,1), y_gen)\n",
    "ridge_modN_pred=ridge_modN.predict(rng.reshape(-1,1))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='red')\n",
    "plt.plot(rng,ridge_modN_pred, color='blue' )\n",
    "\n",
    "ridge_modN['ridgecv'].coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f474f17-29e0-4490-834d-a660ae52a40f",
   "metadata": {},
   "source": [
    "## LASSO ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ca8e0-9fc2-40cc-87c9-8c6fe7ba2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "Lasso_mod=Lasso(alpha=0.5, fit_intercept=True)#, normalize=True)\n",
    "Lasso_mod.fit(rng.reshape(-1,1), y_gen)\n",
    "Lasso_mod_pred=Lasso_mod.predict(rng.reshape(-1,1),)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='green')\n",
    "plt.plot(rng,Lasso_mod_pred, color='blue' )\n",
    "Lasso_mod.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b3d470-a888-460e-97c5-4a4fa8482a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "lasso_pipeline = pipeline(steps=[('preprocess', Lasso_mod),('model', Lasso)])\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='red')\n",
    "plt.plot(rng,Lasso_modN_prod, color='blue' )\n",
    "\n",
    "Lasso_modN['model'].coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3472be65-6f62-4959-ae04-507c0a378b0f",
   "metadata": {},
   "source": [
    "## Elastic Net regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11a344-26f3-4692-8f11-d2dd60b1c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "en_model=ElasticNet(alpha=0.5, fit_intercept=True)#, normalize=True)\n",
    "en_model.fit(rng.reshape(-1,1), y_gen)\n",
    "en_model_pred=en_model.predict(rng.reshape(-1,1),)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(rng, y_gen, color='orange')\n",
    "plt.plot(rng,en_model_pred, color='blue' )\n",
    "en_model.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c9e69-b016-4130-bdc2-24521a47e411",
   "metadata": {},
   "source": [
    "### When to use Ridge, Lasso or Elasticnet? ###\n",
    "<ul>\n",
    "<li><b>Ridge Regression</b> can't zero out coefficient, you either end up including all the coefficients in the model, or none of them.</li>\n",
    "<li><b>LASSO</b> does both parameters shrinkage and variable selection automatically.</li>\n",
    "<li>if some of your covariates are highly correlated, you may want to look at the <b>ElasticNet</b> instead of <b>LASSO</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac91fa7-2a00-4c07-a7d0-7d04afc91ffc",
   "metadata": {},
   "source": [
    "# Polynomial Regression #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259338b-2b84-4b09-9841-2c0e9db21f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d348b7f-e832-462c-baa9-7417be7df0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "\n",
    "print('python:',sys.version)\n",
    "print('Numpy:',np.__version__)\n",
    "print('Pandas:',pd.__version__)\n",
    "print('Scikitlearn:',sklearn.__version__)\n",
    "print('Seaborn: ',sns.__version__)\n",
    "print('matplotlib:',matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab8881-46f7-440d-a78d-c06ff0f5b508",
   "metadata": {},
   "source": [
    "## $$ y= x^3+100+\\epsilon $$ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39d1bb-9d5c-4d60-8140-7ec31a47414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples=100\n",
    "X=np.linspace(0,10,100)\n",
    "rng=np.random.randn(n_samples)*100\n",
    "Y=X**3+100+rng\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X,Y,c='#7f7f7f' )\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff32273-490e-42d9-af8b-cfbe451cc151",
   "metadata": {},
   "source": [
    "### Linear Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb9ff0-0809-4c75-89d8-7889fe68d82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "lr=LinearRegression()\n",
    "lr.fit(X.reshape(-1,1),Y)\n",
    "lr_pred=lr.predict(X.reshape(-1,1))\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X,Y,c='#139fe8' )\n",
    "plt.plot(X,lr_pred, c='#8a010d' )\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(f'model Coef={lr.coef_}, R² ={r2_score(Y,lr_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f42a18-a8a2-4772-b117-7b6abc0df70c",
   "metadata": {},
   "source": [
    "### Polynomial Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ae8f2-c3e4-4420-9828-11c6ea9886a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_reg=PolynomialFeatures(degree=2)\n",
    "X_poly=poly_reg.fit_transform(X.reshape(-1,1))\n",
    "lr2=LinearRegression()\n",
    "lr2.fit(X_poly,Y.reshape(-1,1))\n",
    "Y_pred=lr2.predict(X_poly)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X,Y,c='#14047b' )\n",
    "plt.plot(X,Y_pred, c='red' )\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(f'model Coef={lr2.coef_}, R² ={r2_score(Y,Y_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2f257-a16a-4d68-968d-14b4a8486fac",
   "metadata": {},
   "source": [
    "### Example: Boston dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b47971-dade-4a2d-919f-83bbe9f1d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "Boston=pd.read_csv('housing.data',delim_whitespace=True, header=None)\n",
    "Boston.columns=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX',\n",
    "                'PTRATIO','B','LSTAT','MEDV']\n",
    "\n",
    "pd.options.display.float_format='{:,.3f}'.format\n",
    "sns.pairplot(Boston, size=1.5)\n",
    "Boston.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7260c65f-0f92-427b-857f-f453a9546e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_boston=np.array(Boston['DIS'])\n",
    "Y_boston=np.array(Boston['NOX'])\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_boston,Y_boston)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2dd493-9d7a-4a5e-83b4-bff59a934151",
   "metadata": {},
   "source": [
    "### linear Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711fee2-aafd-4380-be3d-9af98047aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin=LinearRegression()\n",
    "lin.fit(X_boston.reshape(-1,1),Y_boston)\n",
    "lin_pred=lin.predict(X_boston.reshape(-1,1))\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X_boston,Y_boston,c='darkslateblue' )\n",
    "plt.plot(X_boston,lin_pred, c='fuchsia' )\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(f'model Coef={lin.coef_}, R² ={r2_score(Y_boston,lin_pred):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d2191-2191-406d-abc3-144fd9d35e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly=PolynomialFeatures(degree=2)\n",
    "X_poly=poly.fit_transform(X_boston.reshape(-1,1))\n",
    "poly2=LinearRegression()\n",
    "poly2.fit(X_poly,Y_boston.reshape(-1,1))\n",
    "X_fit=np.arange(X_boston.min(), X_boston.max(),1)[:,np.newaxis]\n",
    "Y_pred=poly2.predict(poly.fit_transform(X_fit.reshape(-1,1)))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_boston,Y_boston, c='forestgreen')\n",
    "plt.plot(X_fit,Y_pred,c='lightsalmon', linewidth=3)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(f'model Coef={poly2.coef_}, R² ={r2_score(Y_boston,poly2.predict(X_poly)):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31297e-5e6a-4052-b314-67d51860569c",
   "metadata": {},
   "source": [
    "### Cubic Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a561fdf2-1b34-442a-a98c-7e00db90fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_3=PolynomialFeatures(degree=3)\n",
    "X_poly=poly_3.fit_transform(X_boston.reshape(-1,1))\n",
    "poly3=LinearRegression()\n",
    "poly3.fit(X_poly,Y_boston.reshape(-1,1))\n",
    "X_fit=np.arange(X_boston.min(), X_boston.max(),1)[:,np.newaxis]\n",
    "Y_pred3=poly3.predict(poly_3.fit_transform(X_fit.reshape(-1,1)))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_boston,Y_boston, c='forestgreen')\n",
    "plt.plot(X_fit,Y_pred3,c='lightsalmon', linewidth=3)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(f'model Coef={poly3.coef_}, R² ={r2_score(Y_boston,poly3.predict(X_poly)):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5af61-a34e-434b-84fc-2837c1aa550d",
   "metadata": {},
   "source": [
    "# Non linear relationships #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543e33e-afe0-4b56-8913-c1cc7134a896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682aab30-efe0-4083-8b85-6352d4c469e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Data=pd.read_csv('Data/housing.data',delim_whitespace=True, header=None)\n",
    "Boston=pd.DataFrame(data=np.array(Data),index=None, columns=('CRIM','ZN','INDUS','CHAS',\n",
    "                                                             'NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV'), )#.iloc[:,:-1]\n",
    "\n",
    "#sns.pairplot(Boston, height=1.5)\n",
    "#plt.savefig('CorrMatrix.png')\n",
    "#pd.options.display.float_format='{:,.3f}'.format\n",
    "#Boston.corr()\n",
    "Boston\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45422694-22c7-4f4a-80c7-11accb130c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "Y=Boston['MEDV']\n",
    "X=Boston['LSTAT'].values\n",
    "tree=DecisionTreeRegressor(max_depth=5)\n",
    "tree.fit(X.reshape(-1,1),Y) \n",
    "sort_idx=X.flatten().argsort()\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[sort_idx],Y[sort_idx], c='b')\n",
    "plt.plot(X[sort_idx],tree.predict(X[sort_idx].reshape(-1,1)), color='r', linewidth=2)\n",
    "plt.xlabel('STAT')\n",
    "plt.ylabel('MEDV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9de82d-7bf8-43d7-8b8d-cdf5d0a79e8f",
   "metadata": {},
   "source": [
    "## Random Forest ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504dddd-eb7b-4587-a5aa-b36b8961b952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bee49c-d94c-49fc-8ea2-2f80e924beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=Boston\n",
    "Y=Boston['MEDV']\n",
    "X_train, X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,train_size=0.7, random_state=42) \n",
    "#Criterion: 'poisson', 'squared_error', 'friedman_mse', 'absolute_error'\n",
    "forest=RandomForestRegressor(n_estimators=500, criterion='friedman_mse', random_state=42, n_jobs=-1)\n",
    "forest.fit(X_train, Y_train)\n",
    "Y_train_pred=forest.predict(X_train)\n",
    "Y_test_pred=forest.predict(X_test)\n",
    "print(f'MSE Train={mean_squared_error(Y_train,Y_train_pred):.4f} , MSE Test={mean_squared_error(Y_test,Y_test_pred):.4f}')\n",
    "print(f'R² Train={r2_score(Y_train,Y_train_pred):.4f} , R² Test={r2_score(Y_test,Y_test_pred):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88844488-d4f4-455f-b464-401c130c5243",
   "metadata": {},
   "source": [
    "## AdaBoost ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc0cd3-a80e-4b0f-a605-566970a98c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ada=AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=500, random_state=42)\n",
    "ada.fit(X_train,Y_train)\n",
    "Y_train_predict=ada.predict(X_train)\n",
    "Y_test_predict=ada.predict(X_test)\n",
    "print(f'MSE Train={mean_squared_error(Y_train,Y_train_pred):.4f} , MSE Test={mean_squared_error(Y_test,Y_test_pred):.4f}')\n",
    "print(f'R² Train={r2_score(Y_train,Y_train_pred):.4f} , R² Test={r2_score(Y_test,Y_test_pred):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1fb18e-6cef-4f7c-9ba2-296d4f5973c6",
   "metadata": {},
   "source": [
    "### Feature importance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8958f-c5d3-4c89-a832-aacca9cb52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X=Boston\n",
    "Y=Boston['MEDV']\n",
    "X_train, X_test,Y_train,Y_test=train_test_split(X,Y, test_size=0.3, random_state=42) \n",
    "tree=DecisionTreeRegressor(max_depth=5)\n",
    "tree.fit(X,Y) #reshape(-1,1)\n",
    "#sort_idx=X.flatten().argsort()\n",
    "\n",
    "Y_train_pred=tree.predict(X_train)\n",
    "Y_test_pred=tree.predict(X_test)\n",
    "print(f'MSE Train={mean_squared_error(Y_train,Y_train_pred):.4f} ,MSE Test={mean_squared_error(Y_test,Y_test_pred):.4f}')\n",
    "print(f'R² Train={r2_score(Y_train,Y_train_pred):.4f} , R² Test={r2_score(Y_test,Y_test_pred):.4f}')\n",
    "\n",
    "result= pd.DataFrame(tree.feature_importances_, Boston.columns)\n",
    "result.columns=['Features']\n",
    "rt=result.sort_values(by='Features', ascending=False)\n",
    "rt.plot(kind='bar', figsize=(10,5), edgecolor='black')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc8984-2492-43a1-8393-068fe1cab6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "forest.feature_importances_\n",
    "result= pd.DataFrame(forest.feature_importances_, Boston.columns)\n",
    "result.columns=['Features']\n",
    "rt=result.sort_values(by='Features', ascending=False)\n",
    "rt.plot(kind='bar', figsize=(10,8), edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bf88b-b8c9-4b71-ad97-7cd58ab44b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ada\n",
    "ada.feature_importances_\n",
    "result= pd.DataFrame(ada.feature_importances_, Boston.columns)\n",
    "result.columns=['Features']\n",
    "rt=result.sort_values(by='Features', ascending=False)\n",
    "rt.plot(kind='bar', figsize=(10,4), edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2cf113-408a-440c-afb4-e368fb8f8c6d",
   "metadata": {},
   "source": [
    "# Data Preprocessing #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e0a4c-3832-4aac-8872-fada44d315b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.read_csv('Data/housing.data',delim_whitespace=True, header=None)\n",
    "Boston=pd.DataFrame(data=np.array(Data),index=None,\n",
    "                    columns=('CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV'), )#.iloc[:,:-1]\n",
    "\n",
    "#sns.pairplot(Boston, height=1.5)\n",
    "#plt.savefig('CorrMatrix.png')\n",
    "#pd.options.display.float_format='{:,.3f}'.format\n",
    "co=Boston.corr()\n",
    "#Boston\n",
    "co['MEDV'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48eef54-39be-4a9e-8ecb-b57e6e73494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=Boston['LSTAT']\n",
    "Y=Boston['MEDV']\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(X,Y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406a68f-2bc6-4da5-8a91-907862a3f9cf",
   "metadata": {},
   "source": [
    "### Without Preprocessing ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b81c0-2b2c-4569-8ddc-0115964631c0",
   "metadata": {},
   "source": [
    "<p>Gradient descent is a widely used machine learning algorithm. It tells us how we can do better in predictive modeling <br>with an iterative approach. We will see how we can use the gradient descent algorithm to get better predicting results in linear regression.</p>\n",
    "<b> cost function in linear regression: </b><br>\n",
    "## $$ \\sum_{i=0}^n(yi-(\\beta1x+\\beta0))^2  $$ ##\n",
    "<p>A cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between x and y. <br>\n",
    "The starting point is known as Learning Rate a constant (generally denoted as alpha ‘α’). The learning rate is a step to go down to attain convergence in minimum steps. At every step, the function will calculate the slope of a line and curve at that certain point. The convergence will attain when the slope will be equal to zero.</p>\n",
    "Gradient descent steps:\n",
    "<ol>\n",
    "<li>Adding a column of ones to x vector: \n",
    "  <b>x=np.c_[np.ones(x_RM.shape[0]),x_RM, x_LSTAT]</b></li>\n",
    "<li>Guess/Random θ: $$ \\theta (n+1*1)--> \\theta_0, \\theta_1,...\\theta_n  $$  <br></li>\n",
    "we have x and θ_0, θ_1...θ_n  , we can do linear regression and predict the error \n",
    "<li>Predict the y values : </li>\n",
    " $$ pred^i=\\theta_0 * 1 + \\sum_{i=0}^n \\theta_i x_i   $$  \n",
    "<li>Calculate the error: calculate MSE </li>\n",
    "<li>Calculate the cost function :</li>\n",
    "  $$  cost=\\frac{1}{2m} \\sum_{i=1}^m (error)^2 $$ \n",
    "<li>Update θ :</li>\n",
    "    $$ \\theta=\\theta - \\alpha \\frac{1}{m} * \\sum_{i=1}^m (error)*x $$ \n",
    "<li>Repeat till the change in the cost function is negligible </li>\n",
    "<ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5754e6-cf9e-402f-a884-cf3a5b1144d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.0001\n",
    "m=Y.size\n",
    "np.random.seed(10)\n",
    "theta=np.random.rand(3)\n",
    "def Gradient_descent(x, y, m, theta, alpha):\n",
    "    cost_list=[]\n",
    "    theta_list=[]\n",
    "    prediction_list=[]\n",
    "    run=True\n",
    "    cost_list.append(1e10)\n",
    "    i=0\n",
    "    while run:\n",
    "        prediction=np.dot(x, theta)\n",
    "        prediction_list.append(prediction)\n",
    "        error=prediction-y\n",
    "        cost=1/(2*m)*np.dot(error.T, error)\n",
    "        cost_list.append(cost)\n",
    "        theta=theta-alpha*(1/m)*np.dot(x.T, error)\n",
    "        theta_list(theta)\n",
    "        if  cost_list[i]-cost_list[i+1] < 1e-9:\n",
    "            run=False\n",
    "        i+=1\n",
    "    cost_list.pop(0)\n",
    "    return prediction_list, cost_list, theta_list\n",
    "Gradient_descent(X, Y, m, theta, alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bcc5a4-8a6f-46a9-8a9d-8e5e8333a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Descent:\n",
    "alpha=0.0001\n",
    "w_=np.zeros(1+X.shape[0])\n",
    "cost_=[]\n",
    "n_=100\n",
    "for i in range (n_):\n",
    "    y_pred=np.dot(X, w_[1:])+w_[0]\n",
    "    errors=Y-y_pred\n",
    "    w_[1:]+=alpha+X.T. dot(errors)\n",
    "    w_[0]+=alpha+errors.sum()\n",
    "    cost=(errors**2).sum()/2.0\n",
    "    cost_.append(cost)\n",
    "    \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, n_+1),cost_)\n",
    "plt.xlabel('SSE')\n",
    "plt.ylabel('Epoch')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
