{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98289d-a0e5-4d0a-a351-c0fb1c060cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import matplotlib\n",
    "\n",
    "print('python:',sys.version)\n",
    "print('Numpy:',np.__version__)\n",
    "print('Pandas:',pd.__version__)\n",
    "print('Scikitlearn:',sklearn.__version__)\n",
    "print('Seaborn: ',sns.__version__)\n",
    "print('matplotlib:',matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fabf00-6674-428c-9abc-403f45a604d2",
   "metadata": {},
   "source": [
    "# I. Logistic Regression #\n",
    "Logistic regression is the go-to linear classification algorithm for 2-class problems. it is easy to implement, easy to understand and gets great results on a wide variety of problems.even when the expectations method has for your data is violated.<br>\n",
    "Logistic regression is named after the function used at the core of the method, the logistic function.\n",
    "The logistic function, also called the Sigmoid function, was developped by statisticians to describe properties of population growth in ecology, rising quicly and and maxing out at the carrying capacity of the environment.\n",
    "It's an S shaped curve that can take any real valued number and map it into value between  0 and 1, and never exactly at those limits.<br>\n",
    "$$ 1\\over 1+e ^{-x} $$ \n",
    "<p> e is the base of the natural logarithms and x is the value that you want to transform via the logistic function.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb6415-3007-4f27-b5d5-9eed68ac16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-6,6,num=1000)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x,1/(1+np.exp(-x)))\n",
    "plt.title('Sigmoid function')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3931c-3255-4938-9f85-dc58dd5724c9",
   "metadata": {},
   "source": [
    "## 1) Logistic regression model ##\n",
    "The coefficients (beta values b) of the logistic regression algorithm must be estimated from your training data.\n",
    "<ul>\n",
    "<li>Generally done using maximum likelihood estimation. </li>\n",
    "<li>maximum likelihood estimation is a common learning alogorithm. </li>\n",
    "<li>Note the underlying assumptions about the distribution of your data. </li>\n",
    "<li>The best coefficients would result in a model that would predict a value very close to 1 (male for example) for the default class, and a value very close to 0 (female for example)for the other class. </li>\n",
    "<li>The intuition for maximum likelihood for logistic regression is that a search procedure seeks values for the coefficients (Beta values) that minimize the error in the probabilities predicted by the model to those in the data. </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7894a5a-7a47-48a2-b4cc-8f8284b2cf83",
   "metadata": {},
   "source": [
    "## 2) Making Predictions ## \n",
    "\n",
    "$$ \\hat{y}= \\frac {1}{1+e^{-\\beta_0-\\beta_1 x}} $$ \n",
    "\n",
    "$ \\beta_0 $ is the intercept term<br>\n",
    "$ \\beta_1 $ is the coefficient for $x_i$\n",
    "\n",
    "$\\hat{y} $ is the predicted output with real value between 0 and 1. To convert this to binary output of 0 or 1, this would either need to be rounded to an integer value or a cutoff point be provided to specify the class segregation point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38ef21-0393-4342-8a2e-b8ba77b71806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data=np.array([[-2.0011,0], [-1.4654,0], [0.0965,0], [1.3881, 0],\n",
    "      [3.0641, 0], [7.6275,1], [5.3324, 1], [6.9225, 1],\n",
    "      [8.7654,1], [7.6737, 1]])\n",
    "x=data[:,0:1]#np.array(data)[:,0:1]\n",
    "y=data[:,1]\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9b77fe-cb8a-4882-b101-c355fe7154ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_log=LogisticRegression(C=1.0, penalty='l2', tol=0.0001)\n",
    "clf_log.fit(x,y)\n",
    "y_pred=clf_log.predict(x)\n",
    "clf_log.predict_proba(x)\n",
    "#clf_log.predict(np.array(5).reshape(1,-1))\n",
    "np.column_stack((y_pred,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520738ea-dce5-4b24-aee4-c8df27fdb1f5",
   "metadata": {},
   "source": [
    "# II. Classification based Machine Learning algorithm #\n",
    "\n",
    "## 1) Scikit-learn definition: ##\n",
    "Supervised learning in which the data comes with additional attributes that we want to predict. The problem can be either:\n",
    "<ul>\n",
    "<li> <b>Classification :</b> samples belong to 2 or more classes and we want to learn from already labeled data how to predict the class of unlabeled data. An example of classification problem would be the handwritting digit recognition example, in which the aim is to assign each input vector to one of finite number of discrete categories.\n",
    "Another way to think of classification is as a discrete (opposed to continuous) form of supervised learning where one has a limited number of categories and for each of the n samples provided, one is to try to label them with the correct label or class. \n",
    "<li> <b>Regression :</b> if the desired output consists of one or more continuous variables, then the task is called regression. An example of a regression problem would be the prediction of the salmon as a function of its age and weight.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef330720-c1dc-4ddf-95fe-8241caa383b5",
   "metadata": {},
   "source": [
    "## 2) MNIST Data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c969b-2ef5-4c9c-b2a0-ae7441f62906",
   "metadata": {},
   "source": [
    " please note that your browser may uncompress these files without telling you. If the files you downloaded have a larger size than the above, they have been uncompressed by your browser. Simply rename them to remove the .gz extension. Some people have asked me \"my application can't open your image files\". These files are not in any standard image format. You have to write your own (very simple) program to read them. The file format is described at the bottom of this page.\n",
    "\n",
    "The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.\n",
    "\n",
    "With some classification methods (particuarly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications.\n",
    "\n",
    "The MNIST database was constructed from NIST's Special Database 3 and Special Database 1 which contain binary images of handwritten digits. NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.\n",
    "\n",
    "The MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint.\n",
    "\n",
    "SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\n",
    "\n",
    "Many methods have been tested with this training set and test set. Here are a few examples. Details about the methods are given in an upcoming paper. Some of those experiments used a version of the database where the input images where deskewed (by computing the principal axis of the shape that is closest to the vertical, and shifting the lines so as to make it vertical). In some other experiments, the training set was augmented with artificially distorted versions of the original training samples. The distortions are random combinations of shifts, scaling, skewing, and compression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b863784-4d07-495a-89ce-a028b51e61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "MNIST=fetch_openml(name='mnist_784' ,data_home = 'Data/MNIST', parser='auto') \n",
    "type(MNIST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1edf539-ebdb-4972-a17e-f7848d9c9c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x,y=MNIST['data'],MNIST['target']\n",
    "x.shape #784 is 28*28\n",
    "type(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd665359-af41-4840-bc05-277a15cd8170",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)\n",
    "x.loc[69999]\n",
    "x2=np.array(x.loc[69999])\n",
    "x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507cc7c-c9cd-4f3c-b81a-4628c04a3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y=y.astype('float')\n",
    "y[69999]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ed6da-d083-4f55-b315-fbd470bd9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(n): \n",
    "    plt.imshow(np.array(x.loc[n]).reshape(28,28))\n",
    "    return\n",
    "viz(69999)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba570a-e108-4038-a913-c54b633a0063",
   "metadata": {},
   "source": [
    "### locating x of number 4 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63940e4a-32e5-4a98-9ec6-3b8946295d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y==4\n",
    "type(y)\n",
    "np.where(y==4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24888159-12ca-4bb6-98d1-886e1a503374",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=np.array(x.loc[69977])\n",
    "_image=_.reshape(28,28)\n",
    "plt.imshow(_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d1fc8-28a7-4216-9881-0bf76f1a7730",
   "metadata": {},
   "source": [
    "### Splitting the Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c162f92-a593-4783-8a08-656416dd7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(x,y, test_size=0.2, shuffle=True, random_state=42)\n",
    "type(Y_test)\n",
    "\n",
    "#2nd method \n",
    "num_split=60000\n",
    "X_train, X_test, Y_train, Y_test= x[:num_split], x[num_split:], y[:num_split], y[num_split:]\n",
    "type(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34984656-2885-4d0b-8306-dc38ba7fe3e7",
   "metadata": {},
   "source": [
    "### Shuffling ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb67b6-7935-454e-b6c8-29a0de5211e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling:\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "num_split=60000\n",
    "\n",
    "#shuffle_index=np.random.permutation(num_split)\n",
    "#X_trainr, Y_trainr=X_train[shuffle_index], Y_train[shuffle_index]\n",
    "X_train, Y_train=shuffle(X_train, Y_train)\n",
    "Y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3dde4e-c48e-4075-98b8-9169debba97f",
   "metadata": {},
   "source": [
    "### training a binary classifier ###\n",
    "to simplify the problem, we make it 2 class problem: 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e6741-c666-47ad-9bb7-9c2cacb12173",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train0=(Y_train==0)\n",
    "Y_test0=(Y_test==0)\n",
    "Y_train0\n",
    "#X_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b9e5f-b4ac-4d75-8c96-68551cfd2b20",
   "metadata": {},
   "source": [
    "## 3) SGDC Classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f4998a-4edc-4e65-824f-60770a53da81",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent Classifier: linear classifiers(SVM, logistic regression) with SGD training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1676c9c4-4205-4229-a2a2-f8a55a0d3540",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04917e37-387a-4d8f-9b96-8aaf9c7faa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without binary Classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf=SGDClassifier(random_state=0)\n",
    "clf.fit(X_train,Y_train)\n",
    "clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323bffd1-9209-4bd9-9c9b-9c8dd8e25596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With binary Classifier:\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "\n",
    "clf=SGDClassifier(random_state=0)\n",
    "#Y_train0 = column_or_1d(Y_train0, warn=True)\n",
    "clf.fit(X_train,Y_train0) #.reshape(1,-1)\n",
    "# ça ne marche pas !!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955128c6-cc0b-4a63-a1a2-5423850da741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_n=np.array(x.loc[1001]).reshape(1,-1)\n",
    "clf.predict(x_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb39ae4-4832-47cb-8a82-c4407aa05e18",
   "metadata": {},
   "source": [
    "## 4) Performance Measure ##\n",
    "### Cross Validation: Stratified K fold ###\n",
    "Stratified K fold uses stratified sampling to create multiple folds.At each iteration, the classifier is cloned and trained using the training folds and makes predictions on the test fold. <br> \n",
    "Stratified K fold utilise the stratified sampling concept:\n",
    "<ul>\n",
    "<li>The population is divided into homogeneous sub groups called strata.</li>\n",
    "<li>The right number of instances is sampled from each stratum.</li>\n",
    "<li>To guarantee that the test set is representative of the population.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efabc197-49f4-44fc-8977-1e53486f3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(x,y, test_size=0.2, shuffle=True,random_state=0)\n",
    "                                                   #stratify=MNIST['data'])\n",
    "\n",
    "clf=SGDClassifier(random_state=0)\n",
    "\n",
    "Y_train0=(Y_train==0)\n",
    "Y_test0=(Y_test==0)\n",
    "Y_train0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6aaff9-86a6-446a-a512-f4febb641a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "skfolds=StratifiedKFold(n_splits=5, shuffle=True, random_state=100)\n",
    "for train_index, test_index in skfolds.split(X_train, Y_train):\n",
    "    clone_clf=clone(clf)\n",
    "    X_train_fold,X_test_fold=X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    Y_train_fold,Y_test_fold=Y_train.iloc[train_index],Y_train.iloc[test_index]\n",
    "    \n",
    "clone_clf.fit(X_train_fold, Y_train_fold)\n",
    "clf.fit(X_train_fold, Y_train_fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dddba0-c16d-4b54-a445-1bc9ccac8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=clone_clf.predict(X_test_fold)\n",
    "n_correct=sum(Y_pred==Y_test_fold)\n",
    "print(f'{(n_correct)/len(Y_pred):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5365513b-03e5-481d-9337-af42aaf15353",
   "metadata": {},
   "source": [
    "### Cross Validation: cross_val_score ###\n",
    "kfold cross validation splits the training set into k-folds and then make predictions and evaluate them on each fold using a model trained on the remaining folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce531e-0321-45fe-bd4f-fb6eb3775ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(clf, X_train, Y_train, cv=4, scoring='accuracy' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71e466-970f-4203-9fe9-ed682c52f4e8",
   "metadata": {},
   "source": [
    "### Danger of blindly applying evaluator as a performance measure ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e153bc8-64ba-4f79-a37f-592d54004ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(1-sum(Y_train0))/len(Y_train0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cee5ea-6492-4899-a72c-80c458fa89fc",
   "metadata": {},
   "source": [
    "A simple check shows that 90.1% of the images are not 0, any time you guess that the image is not 0 , <br>you'll be right 90.13% of the time.<br>\n",
    "Bear this in mind when you're dealing with skewed datasets. Because of this, accuracy is generlly not the preferred performance measure for classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b4a0e-2aae-42ae-ab07-ab9f27ecaa97",
   "metadata": {},
   "source": [
    "## 5) Confusion Matrix ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62783e75-c326-48e8-91fc-12707c26eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_train_predict=cross_val_predict(clf, X_train, Y_train, cv=3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca3c2e-5b5a-49c1-b746-798b946d6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_train, Y_train_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ddb0ba-ad6f-4240-b046-91f6efe2987d",
   "metadata": {},
   "source": [
    "Confusion Matrix is a performance measurement for machine learning classification problem<br> where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values.\n",
    "It is extremely useful for measuring Recall, Precision, Specificity, Accuracy, and most importantly AUC-ROC curves.\n",
    "Let’s understand TP, FP, FN, TN in terms of pregnancy analogy.<br>\n",
    "<img src='cott.png' width=50% /> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b5bcb-23ae-473b-adb8-f59a02da5b41",
   "metadata": {},
   "source": [
    "<ul> \n",
    "<li> <b>True Positive:</b> <br> \n",
    "Interpretation: You predicted positive and it’s true.<br>\n",
    "You predicted that a woman is pregnant and she actually is.</li>\n",
    "<li><b>True Negative:</b><br>\n",
    "Interpretation: You predicted negative and it’s true.<br>\n",
    "You predicted that a man is not pregnant and he actually is not.<br>\n",
    "</li>\n",
    "<li> <b>False Positive: (Type 1 Error)</b> <br>\n",
    "Interpretation: You predicted positive and it’s false.<br>\n",
    "You predicted that a man is pregnant but he actually is not.<br>\n",
    "<li><b>False Negative: (Type 2 Error)</b><br>\n",
    "Interpretation: You predicted negative and it’s false.<br>\n",
    "You predicted that a woman is not pregnant but she actually is.<br>\n",
    "</li>\n",
    "</ul>\n",
    "Just Remember, We describe predicted values as Positive and Negative and actual values as True and False.\n",
    "\n",
    "### Recall : ###\n",
    "\n",
    "$$ Recall=\\frac{TP} {TP+FN} $$\n",
    "\n",
    "The above equation can be explained by saying, from all the positive classes, how many we predicted correctly.<br>\n",
    "Recall should be high as possible.\n",
    "\n",
    "### Precision : ###\n",
    "$$ Precision=\\frac{TP} {TP+FP} $$\n",
    "\n",
    "The above equation can be explained by saying, from all the classes we have predicted as positive, how many are actually positive.<br>\n",
    "Precision should be high as possible.\n",
    "and\n",
    "\n",
    "### Accuracy : ###\n",
    "\n",
    "$$ Precision=\\frac{TP+TN} {TP+TN+FP+FN} $$\n",
    "From all the classes (positive and negative), how many of them we have predicted correctly. In this case, it will be 4/7.\n",
    "\n",
    "Accuracy should be high as possible.\n",
    "\n",
    "### F-measure: ###\n",
    "\n",
    "$$ F-measure=\\frac{2*Recall*Precision} {Recall+Precision} $$ <br>\n",
    "It is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1fe51-6639-4140-8939-657ef035312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "#precision_score(Y_train, Y_train_predict, average=None)\n",
    "recall_score(Y_train, Y_train_predict, average=None)\n",
    "f1_score(Y_train, Y_train_predict,average='weighted')#average=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b71a96-25e3-4156-9ae7-9f7b584bcabc",
   "metadata": {},
   "source": [
    "## 6) Precision Recall tradeoff ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac03e1-d103-4faa-83d6-acd1d4b00b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(x,y, test_size=0.3,\n",
    "                                                   shuffle=True,random_state=0)\n",
    "                                                  \n",
    "np.random.seed(0)\n",
    "clf=SGDClassifier(random_state=0)\n",
    "clf.fit(X_train,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de11ff42-9032-4857-ba30-c991cbcf51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_train0=[]\n",
    "for a in Y_train.values:\n",
    "    if a=='0':\n",
    "        Y_train0.append(0)\n",
    "    else: Y_train0.append(1)\n",
    "Y_train0\n",
    "np.unique(Y_train0, return_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34020177-be83-4229-93c5-d1bd6f66e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=np.array(X_train.loc[1001])\n",
    "Y_scores=clf.decision_function(x2.reshape(1,-1))\n",
    "#Y_scores=clf.decision_function(x.loc[1001].reshape(1,-1))\n",
    "Y_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba0e03-1571-48f1-93d3-d2078157bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0\n",
    "y_some_digits_pred=(Y_scores>threshold)\n",
    "y_some_digits_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a913a-18e8-4030-a661-1ffff7fd371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "\n",
    "Y_scores=cross_val_predict(clf, X_train,Y_train0, cv=3, method='decision_function')\n",
    "Y_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd6912-a641-4090-a3a0-960da23e79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors / colormaps on Matplotlib\n",
    "import matplotlib.colors\n",
    "\n",
    "palette = plt.colormaps['tab20b'].resampled(10)\n",
    "palette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e2e822-1454-4c90-aa05-dc854f4cfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(Y_scores, bins=10)#, color=palette)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d7fc7-81b3-4e17-9c58-b14e71935f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds= precision_recall_curve(y_true=Y_train0, probas_pred=Y_scores,\n",
    "                                                        pos_label=None)\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1],'b--',label='Precision')\n",
    "    plt.plot(thresholds, recalls[:-1],'g .',label='Recall')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim([-0.1,1.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a4e28-ff2d-4fef-b9f2-0548ea615308",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268b6fe4-767c-41fa-b4d5-06a79a60aea3",
   "metadata": {},
   "source": [
    "Some tasks may call for <b>higher precision (accuracy of positive predictions)</b>, like designing a classifier that picks up<br> adult content to protect children, this will require the classifier to set a high bar to allow any contents to be consumed by children.<br>\n",
    "Some tasks may call for <b>higher Recall (ratio of positive instances that are correctly detected by the classifier)</b>, such as detecting shoplifters / intruders on surveillance images. Anything that remotely resemble POSITIVE instances to be picked up. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29147ae6-5651-4d0a-90da-7ac0e1468965",
   "metadata": {},
   "source": [
    "## 7) Altering then Precision-Recall tradeoff ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c776d-f7e8-4bc3-a8f2-9741af16c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting high precision\n",
    "len(precisions),len(thresholds)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(thresholds, precisions[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9e8e8-9218-42fc-bdc8-7064365c1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "idx=len(precisions[precisions<0.95])\n",
    "thresholds[idx]\n",
    "Y_train_pred_90=Y_scores>thresholds[idx]\n",
    "precision_score(Y_train0, Y_train_pred_90), recall_score(Y_train0, Y_train_pred_90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78e4a7-ad35-481a-8f11-ca4a9c438a14",
   "metadata": {},
   "source": [
    "## 8) The Receiver Operating Characteristics Curve (ROC) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166a7b6-61ca-4ce6-bd2e-a6fd5d96c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "#fpr:false positive rate; tpr: true positive rate\n",
    "fpr, tpr, thresholds=roc_curve(Y_train0, Y_scores)\n",
    "label=None\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(fpr, tpr,'b--', linewidth=2,label=label)\n",
    "plt.plot([0,1], [0,1],'k .',label='Recall')\n",
    "#plt.axis(0,1,0,1)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d15bd-859c-406c-9af8-6a1ce4df64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(Y_train0, Y_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd23667d-96bf-4b8b-907b-8620c043be24",
   "metadata": {},
   "source": [
    "Use <b>PR(Precision-Recall) </b>curve whenever <b>the positive class is rare,</b> or when you care more<br> about the <b>false positives </b>than the false negatives.<br>\n",
    "Use <b>ROC</b> curve whenever the<b> negative class is rare ,</b> or when you care more about the<b> false negatives </b>than the false positives.<br><br>\n",
    "In the latest example, ROC curve seems to suggest that the classifier is good. However, when you look to PR curve, you can see that there are room for improvement ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e8cfe-6fcd-40da-a24f-051a529b29ac",
   "metadata": {},
   "source": [
    "## 9) The Random Forest Classifier ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c099030-cf8e-432e-a130-2fae0039ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "\n",
    "rf_c=RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "Y_proba_forest=cross_val_predict(rf_c,X_train,Y_train0, cv=3, method='predict_proba')\n",
    "Y_scores_forest=Y_proba_forest[:,1]\n",
    "fpr_forest,tpr_forest,threshold_forest=roc_curve(Y_train0, Y_scores_forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85cc17-e921-4f5e-acc1-43970c3ff42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(fpr,tpr, 'b v', label='SGD')\n",
    "\n",
    "plt.plot(fpr_forest,tpr_forest,'r .',label='Random Forest' )\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f877d-696a-4944-89b2-8576d26491c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(Y_train0, Y_scores), roc_auc_score(Y_train0, Y_scores_forest)\n",
    "rf_c.fit(X_train,Y_train0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf33daf-88b3-486a-be00-fe2100d6b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Cross Validation:\n",
    "Y_train_RF=cross_val_predict(rf_c,X_train,Y_train0, cv=3)\n",
    "p=precision_score(Y_train0, Y_train_RF)\n",
    "q=recall_score(Y_train0, Y_train_RF)\n",
    "r=confusion_matrix(Y_train0, Y_train_RF)\n",
    "print(f'Precision score={p} \\n Recall Score= {q} \\n Confusion Matrix= {r}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
