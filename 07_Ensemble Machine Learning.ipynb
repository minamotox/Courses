{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3eee687-4ddf-47a7-8d37-6b18de5d0d65",
   "metadata": {},
   "source": [
    "# Ensemble Machine Learning #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7cca29-9f75-4423-b1a4-5f8fbcedafdd",
   "metadata": {},
   "source": [
    "# I. Ensemble Learning Methods #\n",
    "Supervised learning\n",
    "Group of predictors (Classifiers/regressors)\n",
    "\n",
    "<b>Bootstrap Aggregating or Bagging</b>\n",
    "<ul>\n",
    "<li>Bootstrap sampling: sampling with replacement.</li>\n",
    "<li>Combine by averaging the output.</li>\n",
    "<li>Combine by voting.</li>\n",
    "<li>Can be applied to many classifiers which includes ANN, CART, ...</li>\n",
    "</ul> \n",
    "<b>Pasting</b>\n",
    "<ul>\n",
    "<li>Sampling without replacement</li>\n",
    "</ul>\n",
    "<b>Boosting </b>\n",
    "<ul>\n",
    "<li>Train week Classifiers</li>\n",
    "<li>Add them to a final strong classifier by weighting. Weighting by accuracy(typically)</li>\n",
    "<li>Once added, the data is reweighted:</li>\n",
    "<ul><li>misclassified classes gain weight.</li>\n",
    "<li> Correctly classified samples lose weight (Exception: boost by majority and Brownboost - decrese the weight of repeatedly misclassified examples)</li>\n",
    "<li>Algo are forced to learn more from misclassified samples.</li>\n",
    "</ul></ul>\n",
    "<b>Stacking</b>\n",
    "<ul>\n",
    "<li>Also knownas stacked generalization </li>\n",
    "<li>From kaggle: combine information from multiple predictive models to generate a new model. Often times  the stacked model (also called 2nd level model) will outperform each of the individual models due its smooting nature and ability to highlight each  base model where it performs best and discredit each base model where it performs poorly. For this reason, stacking is most effective when the base models are significantly different.</li>\n",
    "<li>Trainin a learning algorithm to combine the predictions of several other learning algorithms.</li>\n",
    "<ul><li>Step1: Train learning algo.</li>\n",
    "<li>Step2: Combiner algo is trained using algo predictions from step1. </li>\n",
    "</ul>\n",
    "    \n",
    "    \n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65751d47-81d4-499f-93e8-af85ddb45a7e",
   "metadata": {},
   "source": [
    "# II. Bagging (Bootstrap aggregating) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac431589-c1b5-4986-83db-898354b74e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import matplotlib\n",
    "\n",
    "print('python:',sys.version)\n",
    "print('Numpy:',np.__version__)\n",
    "print('Pandas:',pd.__version__)\n",
    "print('Scikitlearn:',sklearn.__version__)\n",
    "print('Seaborn: ',sns.__version__)\n",
    "print('matplotlib:',matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd71ca-e6d8-4c60-a559-0d26430fe00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titan=sns.load_dataset('titanic')\n",
    "Titan.shape\n",
    "Titan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a4501-d87f-4bca-bc44-81cbc4789a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Titan.dropna(inplace=True) #drop all NaN samples\n",
    "Titan['pclass'].unique()\n",
    "Titan['pclass'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6119d5-6544-45a2-b310-ccf9f5801359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Titan['sex'].unique()\n",
    "Titan['sex'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e0d5f9-0b77-46f7-a876-d70856cb16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titan['age'].hist(bins=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e21f8-0b31-43e1-b019-abc619ec75d5",
   "metadata": {},
   "source": [
    "## 1) Data pre-processing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366c9ff-acd9-4f34-b8d7-b302cd511e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "Titan2= Titan[['pclass', 'sex', 'age','alive']].copy()\n",
    "\n",
    "LE=preprocessing.LabelEncoder()\n",
    "#LE=preprocessing.LabelBinarizer()\n",
    "Titan2['sex']=LE.fit_transform(Titan['sex'])\n",
    "Titan2['alive']=LE.fit_transform(Titan['alive'])\n",
    "Titan2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03463b2b-375e-4d19-b73a-ee4d118f4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaN (missing) Values:\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(Titan2)\n",
    "\n",
    "Tit3=imp.transform(Titan2)\n",
    "X=np.delete(Tit3,3,1)\n",
    "Y=np.delete(Tit3,[0,1,2],1)\n",
    "X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca00d3b1-f572-412e-8c0c-01ed8b55eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obsolete part\n",
    "from sklearn import preprocessing\n",
    "X= Titan[['pclass', 'sex', 'age']].copy()\n",
    "LE=preprocessing.LabelEncoder()\n",
    "#LE=preprocessing.LabelBinarizer()\n",
    "X['sex']=LE.fit_transform(Titan['sex'])\n",
    "X.describe()\n",
    "#X.info()\n",
    "Y=Titan['survived']\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47a2d4-0499-43e3-acb7-bf6e6842f9db",
   "metadata": {},
   "source": [
    "## 2) Fit Model: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818f1df-f5e6-4408-8bb9-462e68e40f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.3)\n",
    "#Y_test.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6caf3e-4ddd-490c-8ad7-9d91d664e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "def print_score(clf, X_train, X_test, Y_train, Y_test, train=True):\n",
    "    \"Print the accuracy score, Classification report and confusion matrix\"\n",
    "    LB=preprocessing.LabelBinarizer()\n",
    "    LB.fit(Y_train)\n",
    "    if train:\n",
    "        \"Train performance\"\n",
    "        res=clf.predict(X_train)\n",
    "        \n",
    "        print(f'Train Results:\\n')\n",
    "        print(f'Accuracy Score: %.4f \\n' % (accuracy_score(Y_train,res)))\n",
    "        print(f'Classification report:\\n {classification_report(Y_train,res)} ')\n",
    "        print(f'Confusion Matrix Score: {confusion_matrix(Y_train,res)}')\n",
    "        print(f'ROC AUC Score:%.4f'%(roc_auc_score(LB.transform(Y_train),LB.transform(res))))\n",
    "        \n",
    "        res=cross_val_score(clf,X_train, Y_train, cv=10, scoring='accuracy')\n",
    "        print('Average Accuracy \\t: %.4f'%(np.mean(res)))\n",
    "        print('Accuracy SD \\t: %.4f '% (np.std(res)))\n",
    "                      \n",
    "    elif train==False:\n",
    "        \"Test performance\"\n",
    "        res_test=clf.predict(X_test)\n",
    "        print(f'Test Results:\\n')\n",
    "        print('Accuracy Score: %.4f \\n'%(accuracy_score(Y_test,res_test)))\n",
    "        print(f'Classification report:\\n {classification_report(Y_test,res_test)}')\n",
    "        print(f'Confusion Matrix Score: {confusion_matrix(Y_test,res_test)}')\n",
    "        print('ROC AUC Score: %.4f\\n'%(roc_auc_score(LB.transform(Y_test),LB.transform(res_test))))\n",
    "        print('Average Accuracy \\t: %.4f'%(np.mean(res_test)))\n",
    "        print('Accuracy SD \\t: %.4f '% (np.std(res_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918976fe-53f0-42d7-aa62-55c1a01c321b",
   "metadata": {},
   "source": [
    "### a\\ Decision Tree ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2249ab41-c5c7-456e-9873-0266d7cb2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf=DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d2b64-0955-4579-97e3-c96248334fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(clf, X_train, X_test, Y_train, Y_test, train=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074d332-8f21-43b3-a80b-8aed75659de3",
   "metadata": {},
   "source": [
    "### b/ Bagging (oob_score=False): ###\n",
    "Out of bag error (OOB): or out of bag estimate is a method of measuring the prediction error of random forests, boosted decision trees, and other ML models using bootstrap aggregating (bagging) to sub sample data samples used for training.<br>\n",
    "OOB is the mean prediction error on each training sample $ x_i $, using only the tree that did not have  $ x_i $ in their bootstrap sample.<br>\n",
    "Subsampling allows one to define an out of bag estimate of prediction performance improvement by evaluating predictions on those observations which were not used in the building of the next base learner.<br>\n",
    "OOB estimate helps avoid the need for an independant validation dataset, but often underestimates actual performance improvement and the optimal number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361f981-374e-4194-b999-f49bb8236d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf=BaggingClassifier(estimator=clf, n_estimators=1000, bootstrap=True, \n",
    "                          oob_score=False, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f32542-8c79-475b-958f-b64c435f2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(bag_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d173d319-7939-449c-a927-bc783c86a16b",
   "metadata": {},
   "source": [
    " ### c/ Bagging (oob_score=True): ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447c925-f467-4f13-a61e-9f66739fd04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf=BaggingClassifier(estimator=clf, n_estimators=1000, bootstrap=True, \n",
    "                          oob_score=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7b096-60b5-4579-87e4-f6d8f98a3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(bag_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7bad5-f73c-43e0-a107-f3e41bb29cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OOB Score = %.4f'% (bag_clf.oob_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6fe61-f49d-4972-949c-2b703f467e26",
   "metadata": {},
   "source": [
    " ### d/ Random Forest : ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f7696-a675-4a82-9bd4-98e0ee83dee7",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Ensemble of decision trees. </li>\n",
    "<li>Training via the Bagging method(repeated sampling with replacement).</li>\n",
    "<ul><li>Bagging: sample from samples. </li>\n",
    "<li>RF: Sample from predictors: $ m=\\sqrt(p) $ for classification and $ m=\\frac{p}{3} $ for regression problems.</li>\n",
    "</ul>\n",
    "<li>Utilise uncorrelated trees\n",
    "</ul>\n",
    "<p>\n",
    "Random Forest: sample both observations and features of training data.<br><br>\n",
    "Bagging: <ul>\n",
    "<li>Samples only observations at random.</li>\n",
    "<li>Decision Tree select best feature when splitting a node.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5480d-bafe-4667-90ed-942186689d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rf_clf=RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e8811-5022-4c36-b1f6-6b9886e674b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f' {print_score(rf_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(rf_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de67d63-694b-4269-b5a2-ef71a19a8802",
   "metadata": {},
   "source": [
    " ### e/ Grid Search : ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838f57c-37c5-4321-a0f3-b4f82a37f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid={'max_depth':[3,None], 'min_samples_split':[2,3,10],\n",
    "           'min_samples_leaf':[1,3,10] , 'bootstrap':[True, False] ,\n",
    "           'criterion':['gini', 'entropy']}\n",
    "grid_serc=GridSearchCV(rf_clf,param_grid,n_jobs=-1, cv=5, \n",
    "                       verbose=1, scoring='accuracy', return_train_score=False)\n",
    "grid_serc.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388f4bc-8e13-4a6e-849e-8ffb80bb1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f' {print_score(grid_serc, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(grid_serc, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d49e6-8c2a-4ff4-a5a6-5ca51dbc5a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_serc.best_score_\n",
    "grid_serc.best_estimator_.get_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3379c7-5250-4e5b-a776-d7524973449f",
   "metadata": {},
   "source": [
    " ### f/ Extra Trees Ensemble (Extremely Randomized Trees) : ###\n",
    " <ul><li>Random forest is build upon Decision Tree.</li>\n",
    " <li>Decision tree node splitting is based on gini or entropy or some other algorithms.</li>\n",
    " <li>Extra Trees make use of random thresholds for each feature unlike decision tree.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d7a36-d4be-4a91-a68c-fe64b6eee8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "xt_clf=ExtraTreesClassifier(random_state=42, n_estimators=100)\n",
    "xt_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa446b9-f494-42d0-9701-cdd3c5b74e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f2642-85b9-4fb6-adb3-2dc85835cd76",
   "metadata": {},
   "source": [
    "# III. Boosting: #\n",
    "<br>Combine several weak learners into a strong learner.<br>\n",
    "Train predictors sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868300d2-fc9f-40e1-b167-754cc58ffa5e",
   "metadata": {},
   "source": [
    "## 1) AdaBoost (Adaptive Boosting) (CART): ##\n",
    "<ul><li>Similar to human learning, the algorithm learns from past mistakes by focusing more on difficult problems it did not get right in prior learning.</li>\n",
    "<li>In machine learning language: it pays more attention to training instances that previously underfitted.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0515c60b-5ea7-435c-b4ac-a503a6200678",
   "metadata": {},
   "source": [
    "in Scikit-learn:\n",
    "<ul>\n",
    "<li>Fit a sequence of weak learners (models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of data.</li>\n",
    "<li>The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction.</li>\n",
    "<li>The data modifications at each so called boosting iteration consist in applying weights w1,w2...wn to each of the training samples.</li>\n",
    "<li>Initially, those weights are all set to $w_i=\\frac {1}{N}$, so that the 1st step simply trains a weak learner on the original data.</li>\n",
    "<li>For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data.   </li>\n",
    "<li>At a given step, those training examples that were incorreclty predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. </li>\n",
    "<li>As iterations proceed, examples that are difficult to predict receive ever-increasing influence, each subsequent weak learner is thereby forced to cencentrate on the examples that are missed by the previous ones in the sequence. </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea8c35-e224-4d0a-91c4-aa4c9bacfdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf=AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe0a6d-d79b-469f-99ea-3cd67386639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(ada_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(ada_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69ddf9-b273-498b-a524-98f06c25312f",
   "metadata": {},
   "source": [
    "## 2) AdaBoost with Random Forest: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb52e3-2d4a-48c7-bad5-2606d0124422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ada_clf=AdaBoostClassifier(RandomForestClassifier(n_estimators=100), n_estimators=100)\n",
    "ada_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4da533-907c-4b71-9a07-37325d1a8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(ada_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(ada_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657dd16e-3ef8-4a49-bb58-8a62fddd7b4c",
   "metadata": {},
   "source": [
    "## 3) Gradient Boosting Machine (GBM): ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6eb097-acd2-4d89-8629-3f92364a4465",
   "metadata": {},
   "source": [
    "works with regression and classification\n",
    "<ul><li>Sequentially adding predictors</li>\n",
    "<li>Each one correcting its predecessor</li>\n",
    "<li>Fit a new predictor to the residual errors</li></ul>\n",
    "Compare this to AdaBoost:\n",
    "Alter instance weights at every iteration:\n",
    "<ul>\n",
    "<li>Step 1: $ Y=F(x)+\\epsilon $ </li>\n",
    "<li>Step 2: $ \\epsilon=G(x)+\\epsilon_2 $ </li>\n",
    "<li>Step 3: $ Y=F(x)+G(x)+\\epsilon_2 $ </li>\n",
    "<li>Step 4: $ \\epsilon_2=H(x)+\\epsilon_3 $ </li>\n",
    "<li>Step 5: $ Y=F(x)+G(x)+H(x)+\\epsilon_3 $ </li>\n",
    "<li>Step finale: $ Y=\\alpha F(x)+\\beta G(x)+\\gamma H(x)+\\epsilon_4 $ </li>\n",
    "</ul>\n",
    "Gradient boosting involve 3 elements:\n",
    "<ul>\n",
    "<li>Loss function to be optimized: loss function depends on the type of problem being solved. In the case of regression problems, MSE is used. In classification problems, logarithmic loss will be used. In boosting, at each stage, unexplained loss from prior iterations will be optimized rather than starting from scratch. </li>\n",
    "<li>Weak learner to make predictions: Decision trees are used as weak learner in gradient boosting. </li>\n",
    "<li>Additive model to add weak learners to minimize the loss function: Trees are added one at a time and existing trees in the model are not changed. The gradient descent procedure is used to minimize the loss when adding trees. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83325980-78dc-4737-98a3-e99749fb39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc_clf=GradientBoostingClassifier()\n",
    "gbc_clf.fit(X_train,Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd23ed-3214-4191-a8ca-585a9cd38749",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(gbc_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(gbc_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8fc4a-92f6-4616-9ca0-149cccc812db",
   "metadata": {},
   "source": [
    "## 4) XGBoost(Extreme Gradient Boosting) : ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22633654-9a84-4f33-8cb8-cd2007a1a7b7",
   "metadata": {},
   "source": [
    "### Woking Test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f42988c-3865-41c9-8d88-b7bf081cc4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "data=np.random.rand(100,10)\n",
    "label=np.random.randint(2, size=100)\n",
    "dtrain=xgb.DMatrix(data, label=label)\n",
    "dtrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eaa787-604c-431a-a644-e0cba3c81a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest=dtrain\n",
    "param={'bst:max_depth':2, 'bst_eta':1, 'silent':1, 'objective':'binary:logistic'}\n",
    "param['nthread']=4\n",
    "param['eval_metric']='auc'\n",
    "evallist=[(dtest,'eval'),(dtrain,'train')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6195e54b-0708-498d-a48f-c01c338fdb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round=10\n",
    "bst=xgb.train(param, dtrain, num_round,evallist )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d52801-f4cf-4da3-af2a-78a5c5672ee0",
   "metadata": {},
   "source": [
    "### objective function: training loss + Regularization ###\n",
    "$ obj(\\Theta)=L(\\theta)+ \\gamma (\\Theta) $\n",
    "<ul>\n",
    "<li> L is a training loss function </li>\n",
    "<li> $\\gamma $ is regularization term </li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "### Training Loss: ###\n",
    "It measures how predictve our model is on training data<br>\n",
    "Example1: MSE on linear regression: <br>\n",
    "$ L(\\theta)=\\Sigma_i (y_i-\\hat{y}_i)^2 $ <br>\n",
    "Example2: Logistic loss for logistic regression: <br>\n",
    "$ L(\\theta)=\\Sigma_i [(y_iln(1+e^{-\\hat{y}_i})+ (1-y_i)ln(1+e^{\\hat{y}_i})] $\n",
    "### Regularization term: ###\n",
    "The regularization term controlls the complexity of the model, which helps us to avoid overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8471c2-2282-4f44-8fb8-7aed445e5cb2",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "### Underfitting ###\n",
    "\n",
    "A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data, i.e., it only performs well on training data but performs poorly on testing data. <br>\n",
    "Reasons for Underfitting:\n",
    "<ul>\n",
    "<li>    High bias and low variance. </li>\n",
    "<li>    The size of the training dataset used is not enough. </li>\n",
    "<li>    The model is too simple. </li>\n",
    "<li>    Training data is not cleaned and also contains noise in it. </li>\n",
    "</ul>\n",
    "Techniques to Reduce Underfitting\n",
    "<ul>\n",
    "<li>     Increase model complexity. </li>\n",
    "<li>     Increase the number of features, performing feature engineering. </li>\n",
    "<li>     Remove noise from the data. </li>\n",
    "<li>     Increase the number of epochs or increase the duration of training to get better results. </li>\n",
    "</ul>\n",
    "\n",
    "### Overfitting ###\n",
    "A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. <br>\n",
    "Reasons for Overfitting:\n",
    "<ul>\n",
    "<li>     High variance and low bias. </li>\n",
    "<li>    The model is too complex. </li>\n",
    "<li>    The size of the training data. </li>\n",
    "</ul>\n",
    "Techniques to Reduce Overfitting\n",
    "<ul>\n",
    "<li>    Increase training data. </li>\n",
    "<li>    Reduce model complexity. </li>\n",
    "<li>    Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training). </li>\n",
    "<li>    Ridge Regularization and Lasso Regularization. </li>\n",
    "<li>    Use dropout for neural networks to tackle overfitting. </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c1786b-cd95-4571-83c9-94874ce350c4",
   "metadata": {},
   "source": [
    "### XGBoost VS GBM: ###\n",
    "<ul>\n",
    "<li>Specially, XGBoost is uses a more regularized model formalization to control overfitting, which gives it better performance.</li>\n",
    "<li>For model, it might be more suitable to be called as regularized gradient boosting. </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7a1af-f06e-46af-a9ff-60d43b396604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_clf=xgb.XGBClassifier(max_depth=5, n_estimators=10000, learning_rate=0.3, n_jobs=-1)\n",
    "xgb_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2c071-f1dd-4e51-8cd2-53be553912b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f' {print_score(xgb_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(xgb_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e2f5c-5656-4a31-ad96-43610b59c17d",
   "metadata": {},
   "source": [
    "# IV. HR Project: #\n",
    "Employee attrition is defined as employees leaving their organizations for unpredictable or uncontrollable reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789234d-fce4-455b-b80a-fe304fd5e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HR=pd.read_csv(r'Data/HR-Employee-Attrition.csv')\n",
    "HR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319e02e-ec5c-4ea3-b211-704488f19f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col=list(HR.describe().columns)\n",
    "categorical_col=list(set(HR.columns).difference(num_col))\n",
    "remove_list=['EmplyeeCount','EmplyeeNumber','StandardHours']\n",
    "col_numerical=[e for e in num_col if e not in remove_list]\n",
    "categorical_col.remove('Attrition')\n",
    "#categorical_col\n",
    "#col_numerical.remove('Attrition_num')\n",
    "col_numerical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a02168b-8963-4c1b-a599-04430b26a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Attrition_num={'Yes':1, 'No':0}\n",
    "HR['Attrition_num']=HR['Attrition'].map(Attrition_num)\n",
    "\n",
    "DF_categ=pd.get_dummies(HR[categorical_col])\n",
    "X=pd.concat([HR[col_numerical], DF_categ], axis=1)\n",
    "Y=HR['Attrition_num']\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c964710-20e4-4567-a672-c176529c7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HR[col_numerical].corr()\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(HR[col_numerical].corr(), annot=True, fmt='.3f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a01b5b-e0a7-4ebd-b1c6-7e2859960a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y,random_state=1301,test_size=0.3)\n",
    "#Y_test.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212823e1-7752-48dc-a577-d3988a518c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "def print_score(clf, X_train, X_test, Y_train, Y_test, train=True):\n",
    "    \"Print the accuracy score, Classification report and confusion matrix\"\n",
    "    LB=preprocessing.LabelBinarizer()\n",
    "    LB.fit(Y_train)\n",
    "    if train:\n",
    "        \"Train performance\"\n",
    "        res=clf.predict(X_train)\n",
    "        \n",
    "        print(f'Train Results:\\n')\n",
    "        print(f'Accuracy Score: %.4f \\n' % (accuracy_score(Y_train,res)))\n",
    "        print(f'Classification report:\\n {classification_report(Y_train,res)} ')\n",
    "        print(f'Confusion Matrix Score: {confusion_matrix(Y_train,res)}')\n",
    "        print(f'ROC AUC Score:%.4f'%(roc_auc_score(LB.transform(Y_train),LB.transform(res))))\n",
    "        \n",
    "        res=cross_val_score(clf,X_train, Y_train, cv=10, scoring='accuracy')\n",
    "        print('Cross Val. average accuracy \\t: %.4f'%(np.mean(res)))\n",
    "        print('Cross Val. accuracy SD \\t: %.4f '% (np.std(res)))\n",
    "                      \n",
    "    elif train==False:\n",
    "        \"Test performance\"\n",
    "        res_test=clf.predict(X_test)\n",
    "        print(f'Test Results:\\n')\n",
    "        print('Accuracy Score: %.4f \\n'%(accuracy_score(Y_test,res_test)))\n",
    "        print(f'Classification report:\\n {classification_report(Y_test,res_test)}')\n",
    "        print(f'Confusion Matrix Score: {confusion_matrix(Y_test,res_test)}')\n",
    "        print('ROC AUC Score: %.4f\\n'%(roc_auc_score(LB.transform(Y_test),LB.transform(res_test))))\n",
    "        print('Cross Val. average accuracy \\t: %.4f'%(np.mean(res_test)))\n",
    "        print('Cross Val. average accuracy SD \\t: %.4f '% (np.std(res_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd1de7-f44d-415d-94d8-2b92c00e66cb",
   "metadata": {},
   "source": [
    "## 1) Decision Tree : ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefb563-c031-4bda-b23d-61389f7a23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT_clf=DecisionTreeClassifier(random_state=42)\n",
    "DT_clf.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756aee5-4d6c-41cf-a993-6c3703613099",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(DT_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(DT_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044e47b-8531-4f43-88ee-3e8e81018200",
   "metadata": {},
   "source": [
    "## 2) Bagging : ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969847d1-362b-43c3-b882-b5abf68af18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "Bag_clf=BaggingClassifier(estimator=DT_clf ,n_estimators=100, bootstrap=True,\n",
    "                          n_jobs=-1, random_state=42)\n",
    "Bag_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2325021-541f-4ff0-bcf6-389195cfc4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(Bag_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(Bag_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cc57f-df37-47f5-bd06-7e317217d0a2",
   "metadata": {},
   "source": [
    "## 3) Random Forest : ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac6a5e-9b5d-432d-b24d-a41ec666df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_clf=RandomForestClassifier(n_estimators=100)\n",
    "RF_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95fdde2-c837-4d33-9787-5493075d9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(RF_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(RF_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8b0519-e285-4dcb-bfc1-02b398dfe836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.Series(RF_clf.feature_importances_, index=X_train.columns).sort_values(ascending=False).plot(kind='bar',figsize=(12,6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332f97e-7f58-405e-b5b8-c182d2647778",
   "metadata": {},
   "source": [
    "the use of a forest of trees to evaluate the importance of features on an artificial classification task. The blue bars are the feature importances of the forest, along with their inter-trees variability represented by the error bars(not shown in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f0ea86-d2ee-480b-8ffc-16b76d38a0df",
   "metadata": {},
   "source": [
    "## 4) AdaBoost : ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dbde1b-8c39-4f62-beec-d8ac2cff84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "Ada_clf=AdaBoostClassifier()\n",
    "Ada_clf.fit(X_train, Y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab38c23e-2495-49f0-a312-4687803463c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(Ada_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(Ada_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a47a649-869c-423d-bc6b-a594c6f4b862",
   "metadata": {},
   "source": [
    "## 5) AdaBoost + Random Forest: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641aa3b7-338a-4dac-9db8-39c64a897050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Ada_RF=AdaBoostClassifier(RandomForestClassifier(n_estimators=100), n_estimators=100)\n",
    "Ada_RF.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70b6ca-bfb0-4746-b44e-d94fdab44082",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(Ada_RF, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(Ada_RF, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4be75d-7bdf-463c-b711-a16d5beda5ec",
   "metadata": {},
   "source": [
    "## 6) Gradient Boosting Classifier : ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf32b2-43ab-467f-9794-82a51b1c614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc_clf=GradientBoostingClassifier(n_estimators=100)\n",
    "gbc_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3634d141-0f43-4331-ab5a-a7cadf4f2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(gbc_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(gbc_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd17d700-0355-4119-9525-19456607d75b",
   "metadata": {},
   "source": [
    "## 7) XG Boost : ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ccf2c-8a4d-426b-9585-21629c52cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_clf=xgb.XGBClassifier(n_estimators=100)\n",
    "xgb_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8be08-310e-4f43-ac14-56777954e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(xgb_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(xgb_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf4fe5-2276-416d-9339-497427de4df9",
   "metadata": {},
   "source": [
    "# V. Ensemble of ensembles - Model stacking : #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd1286-3501-4cea-be61-d19ca6bbdc40",
   "metadata": {},
   "source": [
    "<b>Ensemble with different types of classifiers:</b>\n",
    "<ul>\n",
    "<li>Different type of classifiers (logistic regression, decision trees, random forest,...) are fitted on the same training data</li>\n",
    "<li>results are combined based on either:<br>\n",
    "majority voting (classification)<br>\n",
    "average (regression)</li>\n",
    "</ul>\n",
    "<b>Ensemble with a single type of classifier:</b>\n",
    "<ul>\n",
    "<li>Bootstrap samples are drawn with training data.</li>\n",
    "<li>With each bootstrap sample , model (individual model maybe decision trees or random forest) will be fitted.</li>\n",
    "<li>All the results are combined to create an ensemble.</li>\n",
    "<li>Suitable for highly flexible models that are prone to overfitting / high variance. </li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc00cbe6-68e6-466d-adf4-259fabae51cb",
   "metadata": {},
   "source": [
    "## Combining Method ##\n",
    "<ul>\n",
    "<li><b>Majority voting or average:</b><br>\n",
    "Classification: largest number of votes (mode)<br>\n",
    "Regression problems: Average (mean) \n",
    "</li>\n",
    "<li><b>Method of application of meta-classifiers on outcomes: </b><br>\n",
    "Binary outcomes: 0/1 from individual classifiers.<br>\n",
    "Meta classifier is applied on top of the individual classifiers.\n",
    "<li><b>Method of application of meta-classifiers on probabilities:</b><br>\n",
    "Probabilities are obtained from individual classifiers.<br>\n",
    "Applying meta-classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c075b0-0203-404c-963d-e25e33da2ada",
   "metadata": {},
   "source": [
    "## 1) Model1: Decision Tree. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927347bb-b351-4555-814b-398a185a6568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT_clf=DecisionTreeClassifier(random_state=42)\n",
    "DT_clf.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f6ec26-3e20-414f-8da0-4eefd0b2b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(DT_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(DT_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd4d16-efb4-4159-9f92-033a16295ac2",
   "metadata": {},
   "source": [
    "## 2) Model2: Random Forest. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab02754-c681-4d6d-a9ba-7b4021e0afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_clf=RandomForestClassifier(n_estimators=100)\n",
    "RF_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f28d4a-0da6-4c90-93af-311c7429ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(RF_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(RF_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808c946-9580-42dd-b697-5d6ae9548a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_en=pd.DataFrame()\n",
    "DT_clf.predict_proba(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a1a7f-f14b-46a4-b308-0991046a4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_en['DT_clf']= pd.DataFrame(DT_clf.predict_proba(X_train))[1]\n",
    "en_en['RF_clf']= pd.DataFrame(RF_clf.predict_proba(X_train))[1]\n",
    "col_name=en_en.columns\n",
    "#en_en['ind']=pd.DataFrame(Y_train)\n",
    "en_en=pd.concat([en_en['DT_clf'],en_en['RF_clf'],pd.DataFrame(Y_train).reset_index(drop=True)], axis=1)\n",
    "en_en\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc95209-25c8-413a-b621-bdcf61371c96",
   "metadata": {},
   "source": [
    "## 3) Meta Classifier. ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476ab9f-7073-47be-88d0-b9292ea4a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "m_clf=LogisticRegression(fit_intercept=False, solver='lbfgs')\n",
    "Xm=en_en.drop(['Attrition_num'], axis=1)\n",
    "Ym=en_en['Attrition_num']\n",
    "Xm.dropna(inplace=True)\n",
    "Ym.dropna(inplace=True)\n",
    "m_clf.fit(Xm,Ym)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ec267-7f66-4a40-9f6d-4a1de0e975c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_test=pd.DataFrame()\n",
    "en_test['DT_clf']= pd.DataFrame(DT_clf.predict_proba(X_test))[1]\n",
    "en_test['RF_clf']= pd.DataFrame(RF_clf.predict_proba(X_test))[1]\n",
    "en_test['Combined']=m_clf.predict(en_test[['DT_clf','RF_clf']])\n",
    "en_test=pd.concat([en_test['DT_clf'],en_test['RF_clf'],en_test['Combined'],pd.DataFrame(Y_test).reset_index(drop=True)],\n",
    "                axis=1)\n",
    "en_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449aea7-aabb-410b-b15d-b5ada46fdf62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pd.crosstab(en_test['Attrition_num'],en_test['Combined']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e52508-d982-46ab-8797-a957f9d48856",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(accuracy_score(en_test['Attrition_num'],en_test['Combined']),4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018ee2c-3fc1-4700-96c4-524c8dc27812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(en_test['Attrition_num'],en_test['Combined']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e38f19-36ab-4083-9357-c9d69a9a22fa",
   "metadata": {},
   "source": [
    "## 3) Single Classifier. ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a3330-31da-4f6f-9cc9-b59bb3d8bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd10a66-d166-419d-8823-952a8bb051c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HR['Attrition'].value_counts() / HR['Attrition'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c81a1-98eb-4e9b-a3a1-581d946fa54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "\n",
    "class_weight={0:0.838776 , 1:0.161224}\n",
    "pd.Series(list(Y_train)).value_counts()/pd.Series(list(Y_train)).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f7195c-ebac-4308-81fa-2ec8bdd30279",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest=RandomForestClassifier(class_weight=class_weight, n_estimators=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161fef2-2c07-42b1-9522-ae8a40dba2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ada=AdaBoostClassifier(estimator=forest, n_estimators=100, learning_rate=0.5, random_state=42)\n",
    "Ada.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620c185-5a75-45e1-bf4a-fde2c039a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(Ada, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(Ada, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501417f-5ce7-4840-8afb-ee43268ec941",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag_clf=BaggingClassifier(estimator=Ada, n_estimators=50, max_samples=1.0, max_features=1.0,\n",
    "                          bootstrap=True, bootstrap_features=False, n_jobs=-1,\n",
    "                          random_state=42)\n",
    "Bag_clf.fit(X_train, Y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d884e6-ee3d-4e0f-b192-8478d6d735c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' {print_score(Bag_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=True)}')\n",
    "print(f'\\n ---------------------------------------\\n')\n",
    "print(f'{print_score(Bag_clf, X_train, X_test, Y_train.ravel(), Y_test.ravel(), train=False)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ff8a4-9dbb-4deb-a2d0-93c9312e108e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
